{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from typing import Callable, Tuple, Iterable, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "\n",
    "# deepmind related libraries\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import pysindy as ps\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import bandits, disrnn, hybrnn, hybrnn_forget, plotting, rat_data, rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title make update rule of Q-/SINDyNetwork-Agents adjustable and make values of RNN-Agent visible\n",
    "\n",
    "class AgentQuadQ(bandits.AgentQ):\n",
    "  \n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perseveration_bias: float=0.,\n",
    "      ):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "  \n",
    "  def update(self,\n",
    "            choice: int,\n",
    "            reward: float):\n",
    "    \"\"\"Update the agent after one step of the task.\n",
    "\n",
    "    Args:\n",
    "      choice: The choice made by the agent. 0 or 1\n",
    "      reward: The reward received by the agent. 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decay q-values toward the initial value.\n",
    "    self._q = (1-self._forgetting_rate) * self._q + self._forgetting_rate * self._q_init\n",
    "\n",
    "    # Update chosen q for chosen action with observed reward.\n",
    "    self._q[choice] = self._q[choice] - self._alpha * self._q[choice]**2 + self._alpha * reward\n",
    "\n",
    "\n",
    "class AgentSindy(bandits.AgentQ):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perservation_bias: float=0.,):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perservation_bias)\n",
    "\n",
    "    self._update_rule = lambda q, choice, reward: (1 - self._alpha) * q[choice] + self._alpha * reward\n",
    "    self._update_rule_formula = None\n",
    "\n",
    "  def set_update_rule(self, update_rule: callable, update_rule_formula: str=None):\n",
    "    self._update_rule=update_rule\n",
    "    self._update_rule_formula=update_rule_formula\n",
    "\n",
    "  @property\n",
    "  def update_rule(self):\n",
    "    if self._update_rule_formula is not None:\n",
    "      return self._update_rule_formula\n",
    "    else:\n",
    "      return f'{self._update_rule}'\n",
    "\n",
    "  def update(self, choice: int, reward: int):\n",
    "\n",
    "    for c in range(self._n_actions):\n",
    "      self._q[c] = self._update_rule(self._q[c], int(c==choice), reward)\n",
    "\n",
    "\n",
    "class AgentNetwork_VisibleState(bandits.AgentNetwork):\n",
    "\n",
    "  def __init__(self,\n",
    "               make_network: Callable[[], hk.RNNCore],\n",
    "               params: hk.Params,\n",
    "               n_actions: int = 2,\n",
    "               state_to_numpy: bool = False,\n",
    "               habit=False):\n",
    "    super().__init__(make_network=make_network, params=params, n_actions=n_actions, state_to_numpy=state_to_numpy)\n",
    "    self.habit = habit\n",
    "\n",
    "  @property\n",
    "  def q(self):\n",
    "    if self.habit:\n",
    "      return self._state[2], self._state[3]\n",
    "    else:\n",
    "      return self._state[3].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_agents = {\n",
    "    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n",
    "    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    def __init__(self, dataset_type, agent_dict):\n",
    "        self.dataset_type = dataset_type\n",
    "        #self.environment = None\n",
    "        self.agent_dict = agent_dict\n",
    "\n",
    "    def create_dataset(self):\n",
    "        if self.dataset_type == 'synt':\n",
    "            self.setup_synthetic_data()\n",
    "            self.dataset_train, self.experiment_list_train = self.generate_data()\n",
    "            self.dataset_test, self.experiment_list_test = self.generate_data()\n",
    "        \n",
    "        elif self.dataset_type == 'real':\n",
    "            raise NotImplementedError('Real data setup not implemented yet.')\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f'dataset_type {self.dataset_type} not implemented. Please select from drop-down list.')\n",
    "\n",
    "    def setup_synthetic_data(self):\n",
    "        # Define agent parameters\n",
    "        agent_kw = 'basic'  # ['basic', 'quad_q']\n",
    "        gen_alpha = 0.25\n",
    "        gen_beta = 5\n",
    "        forgetting_rate = 0.1\n",
    "        perseveration_bias = 0.0\n",
    "        \n",
    "        # Define environment parameters\n",
    "        non_binary_reward = False\n",
    "        self.n_actions = 2\n",
    "        sigma = 0.1\n",
    "        \n",
    "        # Define experiment parameters\n",
    "        self.n_trials_per_session = 200\n",
    "        self.n_sessions = 220\n",
    "        \n",
    "        # Setup environment and agent\n",
    "        self.environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=self.n_actions, non_binary_rewards=non_binary_reward)\n",
    "        self.agent = self.agent_dict[agent_kw](gen_alpha, gen_beta, self.n_actions, forgetting_rate, perseveration_bias)\n",
    "    \n",
    "    def setup_real_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        return bandits.create_dataset(\n",
    "            agent=self.agent,\n",
    "            environment=self.environment,\n",
    "            n_trials_per_session=self.n_trials_per_session,\n",
    "            n_sessions=self.n_sessions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetCreator(dataset_type='synt', agent_dict=dict_agents)\n",
    "data.create_dataset()\n",
    "n_actions = data.n_actions\n",
    "agent = data.agent\n",
    "\n",
    "dataset_train, experiment_list_train = data.dataset_train, data.experiment_list_train\n",
    "dataset_test, experiment_list_test = data.dataset_test, data.experiment_list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRNN:\n",
    "    def __init__(self, use_hidden_state=False, use_previous_values=False, fit_forget=False, habit_weight=0.0, value_weight=1.0, n_actions=2, hidden_size=16):\n",
    "        # Store parameters\n",
    "        self.use_hidden_state = use_hidden_state\n",
    "        self.use_previous_values = use_previous_values\n",
    "        self.fit_forget = fit_forget\n",
    "        self.habit_weight = float(habit_weight)\n",
    "        self.value_weight = value_weight\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Set up the RNN parameters\n",
    "        self.rnn_rl_params = {\n",
    "            's': self.use_hidden_state,\n",
    "            'o': self.use_previous_values,\n",
    "            'fit_forget': self.fit_forget,\n",
    "            'forget': 0.,\n",
    "            'w_h': self.habit_weight,\n",
    "            'w_v': self.value_weight\n",
    "        }\n",
    "        self.network_params = {\n",
    "            'n_actions': self.n_actions,\n",
    "            'hidden_size': self.hidden_size\n",
    "        }\n",
    "        \n",
    "        # Initialize the model\n",
    "        # self.model = self.make_hybrnn()\n",
    "        \n",
    "    \n",
    "    def make_hybrnn(self):\n",
    "        return hybrnn_forget.BiRNN(rl_params=self.rnn_rl_params, network_params=self.network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "habit_weight=0.0 # used in Sindy RNN\n",
    "rnn = HybridRNN(habit_weight=0.0)\n",
    "optimizer_rnn = optax.adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    def __init__(self, params_path, train=True, load=False, loss_function='categorical'):\n",
    "        self.params_path = params_path\n",
    "        self.train = train\n",
    "        self.load = load\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optax.adam(learning_rate=1e-3)\n",
    "        self.rnn_params = None\n",
    "        self.opt_state = None\n",
    "\n",
    "    def load_parameters(self):\n",
    "        try:\n",
    "            with open(self.params_path, 'rb') as f:\n",
    "                saved_params = pickle.load(f)\n",
    "            self.rnn_params, self.opt_state = saved_params[0], saved_params[1]\n",
    "            print('Loaded parameters.')\n",
    "        except FileNotFoundError:\n",
    "            print('No parameters found to load.')\n",
    "\n",
    "    def save_parameters(self):\n",
    "        with open(self.params_path, 'wb') as f:\n",
    "            pickle.dump((self.rnn_params, self.opt_state), f)\n",
    "        print('Parameters saved.')\n",
    "\n",
    "    def train_model(self, dataset_train, n_steps_max=10000, convergence_thresh=1e-5):\n",
    "        if self.train:\n",
    "            if self.load:\n",
    "                self.load_parameters()\n",
    "            else:\n",
    "                self.rnn_params, self.opt_state = None, None\n",
    "\n",
    "            print('Training the hybrid RNN...')\n",
    "            self.rnn_params, self.opt_state, _ = rnn_utils.fit_model(\n",
    "                model_fun=rnn.make_hybrnn,\n",
    "                dataset=dataset_train,\n",
    "                optimizer=self.optimizer,\n",
    "                optimizer_state=self.opt_state,\n",
    "                model_params=self.rnn_params,\n",
    "                loss_fun=self.loss_function,\n",
    "                convergence_thresh=convergence_thresh,\n",
    "                n_steps_max=n_steps_max\n",
    "            )\n",
    "\n",
    "            self.save_parameters()\n",
    "\n",
    "    def execute(self, dataset_train):\n",
    "        if self.train:\n",
    "            self.train_model(dataset_train)\n",
    "        else:\n",
    "            self.load_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the hybrid RNN...\n",
      "Step 500 of 500; Loss: 0.5348482; Time: 10.6s)\n",
      "Model not yet converged - Running more steps of gradient descent. Time elapsed = 2e-05s.\n",
      "Step 500 of 500; Loss: 0.5346749; Time: 11.1s)\n",
      "Model not yet converged (convergence_value = 0.000323851) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5346544; Time: 11.4s)\n",
      "Model not yet converged (convergence_value = 3.846001e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5346281; Time: 12.8s)\n",
      "Model not yet converged (convergence_value = 4.916381e-05) - Running more steps of gradient descent. Time elapsed = 4e-05s.\n",
      "Step 500 of 500; Loss: 0.5345983; Time: 13.7s)\n",
      "Model not yet converged (convergence_value = 5.574403e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5345682; Time: 11.5s)\n",
      "Model not yet converged (convergence_value = 5.630461e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5345354; Time: 10.7s)\n",
      "Model not yet converged (convergence_value = 6.13253e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5343964; Time: 10.4s)\n",
      "Model not yet converged (convergence_value = 0.0002601467) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5297435; Time: 11.2s)\n",
      "Model not yet converged (convergence_value = 0.008706754) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5295743; Time: 10.8s)\n",
      "Model not yet converged (convergence_value = 0.0003194331) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5295720; Time: 12.5s)\n",
      "Model Converged! Time elapsed = 4e-05s.\n",
      "Parameters saved.\n"
     ]
    }
   ],
   "source": [
    "params_path = 'params/params_rnn_forget_f01_b5.pkl'\n",
    "rnn_train = RNNTrainer(params_path=params_path, train=True, load=False)\n",
    "rnn_train.execute(dataset_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labrotation-daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
