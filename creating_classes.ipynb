{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from typing import Callable, Tuple, Iterable, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "\n",
    "# deepmind related libraries\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import pysindy as ps\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import bandits, disrnn, hybrnn, hybrnn_forget, plotting, rat_data, rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title make update rule of Q-/SINDyNetwork-Agents adjustable and make values of RNN-Agent visible\n",
    "\n",
    "class AgentQuadQ(bandits.AgentQ):\n",
    "  \n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perseveration_bias: float=0.,\n",
    "      ):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "  \n",
    "  def update(self,\n",
    "            choice: int,\n",
    "            reward: float):\n",
    "    \"\"\"Update the agent after one step of the task.\n",
    "\n",
    "    Args:\n",
    "      choice: The choice made by the agent. 0 or 1\n",
    "      reward: The reward received by the agent. 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decay q-values toward the initial value.\n",
    "    self._q = (1-self._forgetting_rate) * self._q + self._forgetting_rate * self._q_init\n",
    "\n",
    "    # Update chosen q for chosen action with observed reward.\n",
    "    self._q[choice] = self._q[choice] - self._alpha * self._q[choice]**2 + self._alpha * reward\n",
    "\n",
    "\n",
    "class AgentSindy(bandits.AgentQ):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perservation_bias: float=0.,):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perservation_bias)\n",
    "\n",
    "    self._update_rule = lambda q, choice, reward: (1 - self._alpha) * q[choice] + self._alpha * reward\n",
    "    self._update_rule_formula = None\n",
    "\n",
    "  def set_update_rule(self, update_rule: callable, update_rule_formula: str=None):\n",
    "    self._update_rule=update_rule\n",
    "    self._update_rule_formula=update_rule_formula\n",
    "\n",
    "  @property\n",
    "  def update_rule(self):\n",
    "    if self._update_rule_formula is not None:\n",
    "      return self._update_rule_formula\n",
    "    else:\n",
    "      return f'{self._update_rule}'\n",
    "\n",
    "  def update(self, choice: int, reward: int):\n",
    "\n",
    "    for c in range(self._n_actions):\n",
    "      self._q[c] = self._update_rule(self._q[c], int(c==choice), reward)\n",
    "\n",
    "\n",
    "class AgentNetwork_VisibleState(bandits.AgentNetwork):\n",
    "\n",
    "  def __init__(self,\n",
    "               make_network: Callable[[], hk.RNNCore],\n",
    "               params: hk.Params,\n",
    "               n_actions: int = 2,\n",
    "               state_to_numpy: bool = False,\n",
    "               habit=False):\n",
    "    super().__init__(make_network=make_network, params=params, n_actions=n_actions, state_to_numpy=state_to_numpy)\n",
    "    self.habit = habit\n",
    "\n",
    "  @property\n",
    "  def q(self):\n",
    "    if self.habit:\n",
    "      return self._state[2], self._state[3]\n",
    "    else:\n",
    "      return self._state[3].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_agents = {\n",
    "    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n",
    "    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    def __init__(self, dataset_type, agent_dict):\n",
    "        self.dataset_type = dataset_type\n",
    "        #self.environment = None\n",
    "        self.agent_dict = agent_dict\n",
    "\n",
    "    def create_dataset(self):\n",
    "        if self.dataset_type == 'synt':\n",
    "            self.setup_synthetic_data()\n",
    "            self.dataset_train, self.experiment_list_train = self.generate_data()\n",
    "            self.dataset_test, self.experiment_list_test = self.generate_data()\n",
    "        \n",
    "        elif self.dataset_type == 'real':\n",
    "            raise NotImplementedError('Real data setup not implemented yet.')\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f'dataset_type {self.dataset_type} not implemented. Please select from drop-down list.')\n",
    "\n",
    "    def setup_synthetic_data(self):\n",
    "        # Define agent parameters\n",
    "        agent_kw = 'basic'  # ['basic', 'quad_q']\n",
    "        gen_alpha = 0.25\n",
    "        gen_beta = 5\n",
    "        forgetting_rate = 0.1\n",
    "        perseveration_bias = 0.0\n",
    "        \n",
    "        # Define environment parameters\n",
    "        non_binary_reward = False\n",
    "        self.n_actions = 2\n",
    "        sigma = 0.1\n",
    "        \n",
    "        # Define experiment parameters\n",
    "        self.n_trials_per_session = 200\n",
    "        self.n_sessions = 220\n",
    "        \n",
    "        # Setup environment and agent\n",
    "        self.environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=self.n_actions, non_binary_rewards=non_binary_reward)\n",
    "        self.agent = self.agent_dict[agent_kw](gen_alpha, gen_beta, self.n_actions, forgetting_rate, perseveration_bias)\n",
    "    \n",
    "    def setup_real_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        return bandits.create_dataset(\n",
    "            agent=self.agent,\n",
    "            environment=self.environment,\n",
    "            n_trials_per_session=self.n_trials_per_session,\n",
    "            n_sessions=self.n_sessions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetCreator(dataset_type='synt', agent_dict=dict_agents)\n",
    "data.create_dataset()\n",
    "n_actions = data.n_actions\n",
    "agent = data.agent\n",
    "\n",
    "dataset_train, experiment_list_train = data.dataset_train, data.experiment_list_train\n",
    "dataset_test, experiment_list_test = data.dataset_test, data.experiment_list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRNN:\n",
    "    def __init__(self, use_hidden_state=False, use_previous_values=False, fit_forget=False, habit_weight=0.0, value_weight=1.0, n_actions=2, hidden_size=16):\n",
    "        # Store parameters\n",
    "        self.use_hidden_state = use_hidden_state\n",
    "        self.use_previous_values = use_previous_values\n",
    "        self.fit_forget = fit_forget\n",
    "        self.habit_weight = float(habit_weight)\n",
    "        self.value_weight = value_weight\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Set up the RNN parameters\n",
    "        self.rnn_rl_params = {\n",
    "            's': self.use_hidden_state,\n",
    "            'o': self.use_previous_values,\n",
    "            'fit_forget': self.fit_forget,\n",
    "            'forget': 0.,\n",
    "            'w_h': self.habit_weight,\n",
    "            'w_v': self.value_weight\n",
    "        }\n",
    "        self.network_params = {\n",
    "            'n_actions': self.n_actions,\n",
    "            'hidden_size': self.hidden_size\n",
    "        }\n",
    "        \n",
    "        # Initialize the model\n",
    "        # self.model = self.make_hybrnn()\n",
    "        \n",
    "    \n",
    "    def make_hybrnn(self):\n",
    "        return hybrnn_forget.BiRNN(rl_params=self.rnn_rl_params, network_params=self.network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "habit_weight=0.0 # used in Sindy RNN\n",
    "rnn = HybridRNN(habit_weight=0.0)\n",
    "optimizer_rnn = optax.adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    def __init__(self, params_path, train=True, load=False, loss_function='categorical'):\n",
    "        self.params_path = params_path\n",
    "        self.train = train\n",
    "        self.load = load\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optax.adam(learning_rate=1e-3)\n",
    "        self.rnn_params = None\n",
    "        self.opt_state = None\n",
    "\n",
    "    def load_parameters(self):\n",
    "        try:\n",
    "            with open(self.params_path, 'rb') as f:\n",
    "                saved_params = pickle.load(f)\n",
    "            self.rnn_params, self.opt_state = saved_params[0], saved_params[1]\n",
    "            print('Loaded parameters.')\n",
    "        except FileNotFoundError:\n",
    "            print('No parameters found to load.')\n",
    "\n",
    "    def save_parameters(self):\n",
    "        with open(self.params_path, 'wb') as f:\n",
    "            pickle.dump((self.rnn_params, self.opt_state), f)\n",
    "        print('Parameters saved.')\n",
    "\n",
    "    def train_model(self, dataset_train, n_steps_max=10000, convergence_thresh=1e-5):\n",
    "        if self.train:\n",
    "            if self.load:\n",
    "                self.load_parameters()\n",
    "            else:\n",
    "                self.rnn_params, self.opt_state = None, None\n",
    "\n",
    "            print('Training the hybrid RNN...')\n",
    "            self.rnn_params, self.opt_state, _ = rnn_utils.fit_model(\n",
    "                model_fun=rnn.make_hybrnn,\n",
    "                dataset=dataset_train,\n",
    "                optimizer=self.optimizer,\n",
    "                optimizer_state=self.opt_state,\n",
    "                model_params=self.rnn_params,\n",
    "                loss_fun=self.loss_function,\n",
    "                convergence_thresh=convergence_thresh,\n",
    "                n_steps_max=n_steps_max\n",
    "            )\n",
    "\n",
    "            self.save_parameters()\n",
    "\n",
    "    def execute(self, dataset_train):\n",
    "        if self.train:\n",
    "            self.train_model(dataset_train)\n",
    "        else:\n",
    "            self.load_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the hybrid RNN...\n",
      "Step 500 of 500; Loss: 0.5348482; Time: 10.6s)\n",
      "Model not yet converged - Running more steps of gradient descent. Time elapsed = 2e-05s.\n",
      "Step 500 of 500; Loss: 0.5346749; Time: 11.1s)\n",
      "Model not yet converged (convergence_value = 0.000323851) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5346544; Time: 11.4s)\n",
      "Model not yet converged (convergence_value = 3.846001e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5346281; Time: 12.8s)\n",
      "Model not yet converged (convergence_value = 4.916381e-05) - Running more steps of gradient descent. Time elapsed = 4e-05s.\n",
      "Step 500 of 500; Loss: 0.5345983; Time: 13.7s)\n",
      "Model not yet converged (convergence_value = 5.574403e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5345682; Time: 11.5s)\n",
      "Model not yet converged (convergence_value = 5.630461e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5345354; Time: 10.7s)\n",
      "Model not yet converged (convergence_value = 6.13253e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5343964; Time: 10.4s)\n",
      "Model not yet converged (convergence_value = 0.0002601467) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5297435; Time: 11.2s)\n",
      "Model not yet converged (convergence_value = 0.008706754) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5295743; Time: 10.8s)\n",
      "Model not yet converged (convergence_value = 0.0003194331) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5295720; Time: 12.5s)\n",
      "Model Converged! Time elapsed = 4e-05s.\n",
      "Parameters saved.\n"
     ]
    }
   ],
   "source": [
    "params_path = 'params/params_rnn_forget_f01_b5.pkl'\n",
    "rnn_train = RNNTrainer(params_path=params_path, train=True, load=False)\n",
    "rnn_train.execute(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sindy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sindy_data(\n",
    "    dataset,\n",
    "    agent: bandits.AgentQ,\n",
    "    sessions=-1,\n",
    "    get_choices=True,\n",
    "    # keep_sessions=False,\n",
    "    ):\n",
    "\n",
    "  # Get training data for SINDy\n",
    "  # put all relevant signals in x_train\n",
    "\n",
    "  if not isinstance(sessions, Iterable) and sessions == -1:\n",
    "    # use all sessions\n",
    "    sessions = np.arange(len(dataset))\n",
    "  else:\n",
    "    # use only the specified sessions\n",
    "    sessions = np.array(sessions)\n",
    "    \n",
    "  if get_choices:\n",
    "    n_control = 2\n",
    "  else:\n",
    "    n_control = 1\n",
    "  \n",
    "  # if keep_sessions:\n",
    "  #   # concatenate all sessions along the trial dimensinon -> shape: (n_trials, n_sessions, n_features)\n",
    "  #   choices = np.expand_dims(np.stack([dataset[i].choices for i in sessions], axis=1), -1)\n",
    "  #   rewards = np.expand_dims(np.stack([dataset[i].rewards for i in sessions], axis=1), -1)\n",
    "  #   qs = np.stack([dataset[i].q for i in sessions], axis=1)\n",
    "  # else:\n",
    "  # concatenate all sessions along the trial dimensinon -> shape: (n_trials*n_sessions, n_features)\n",
    "  # choices = np.expand_dims(np.concatenate([dataset[i].choices for i in sessions], axis=0), -1)\n",
    "  # rewards = np.expand_dims(np.concatenate([dataset[i].rewards for i in sessions], axis=0), -1)\n",
    "  # qs = np.concatenate([dataset[i].q for i in sessions], axis=0)\n",
    "  \n",
    "  choices = np.stack([dataset[i].choices for i in sessions], axis=0)\n",
    "  rewards = np.stack([dataset[i].rewards for i in sessions], axis=0)\n",
    "  qs = np.stack([dataset[i].q for i in sessions], axis=0)\n",
    "  \n",
    "  if not get_choices:\n",
    "    raise NotImplementedError('Only get_choices=True is implemented right now.')\n",
    "    n_sessions = qs.shape[0]\n",
    "    n_trials = qs.shape[1]*qs.shape[2]\n",
    "    qs_all = np.zeros((n_sessions, n_trials))\n",
    "    r_all = np.zeros((n_sessions, n_trials))\n",
    "    c_all = None\n",
    "    # concatenate the data of all arms into one array for more training data\n",
    "    index_end_last_arm = 0\n",
    "    for index_arm in range(agent._n_actions):\n",
    "      index = np.where(choices==index_arm)[0]\n",
    "      r_all[index_end_last_arm:index_end_last_arm+len(index)] = rewards[index]\n",
    "      qs_all[index_end_last_arm:index_end_last_arm+len(index)] = qs[index, index_arm].reshape(-1, 1)\n",
    "      index_end_last_arm += len(index)\n",
    "  else:\n",
    "    choices_oh = np.zeros((len(sessions), choices.shape[1], agent._n_actions))\n",
    "    for sess in sessions:\n",
    "      # one-hot encode choices\n",
    "      choices_oh[sess] = np.eye(agent._n_actions)[choices[sess]]\n",
    "      # add choices as control parameter; no sorting required then\n",
    "      # qs_all = np.concatenate([qs[sess, :, i] for i in range(agent._n_actions)], axis=1)\n",
    "      # c_all = np.concatenate([choices[:, sess, i] for i in range(agent._n_actions)], axis=1)\n",
    "      # r_all = np.concatenate([rewards for _ in range(agent._n_actions)], axis=1)\n",
    "      # concatenate all qs values of one sessions along the trial dimension\n",
    "      qs_all = np.concatenate([np.stack([np.expand_dims(qs_sess[:, i], axis=-1) for i in range(agent._n_actions)], axis=0) for qs_sess in qs], axis=0)\n",
    "      c_all = np.concatenate([np.stack([c_sess[:, i] for i in range(agent._n_actions)], axis=0) for c_sess in choices_oh], axis=0)\n",
    "      r_all = np.concatenate([np.stack([r_sess for _ in range(agent._n_actions)], axis=0) for r_sess in rewards], axis=0)\n",
    "  \n",
    "  # get observed dynamics\n",
    "  x_train = qs_all\n",
    "  feature_names = ['q']\n",
    "\n",
    "  # get control\n",
    "  control_names = []\n",
    "  control = np.zeros((*x_train.shape[:-1], n_control))\n",
    "  if get_choices:\n",
    "    control[:, :, 0] = c_all\n",
    "    control_names += ['c']\n",
    "  control[:, :, n_control-1] = r_all\n",
    "  control_names += ['r']\n",
    "  \n",
    "  feature_names += control_names\n",
    "  \n",
    "  print(f'Shape of Q-Values is: {x_train.shape}')\n",
    "  print(f'Shape of control parameters is: {control.shape}')\n",
    "  print(f'Feature names are: {feature_names}')\n",
    "  \n",
    "  # make x_train and control sequences instead of arrays\n",
    "  x_train = [x_train_sess for x_train_sess in x_train]\n",
    "  control = [control_sess for control_sess in control]\n",
    " \n",
    "  return x_train, control, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SINDyTrainerGroundTruth:\n",
    "    def __init__(self, library, dataset_type='synt', threshold=0.01, dt=1, ensemble=False, library_ensemble=False, get_choices=True):\n",
    "        self.library = library\n",
    "        self.dataset_type = dataset_type\n",
    "        self.threshold = threshold\n",
    "        self.dt = dt\n",
    "        self.ensemble = ensemble\n",
    "        self.library_ensemble = library_ensemble\n",
    "        self.get_choices = get_choices\n",
    "\n",
    "    def fit(self, experiment_list_train, agent, custom_lib_functions=None, custom_lib_names=None, poly_order=3):\n",
    "        if self.library == 'custom_lib':\n",
    "            library_datasindy = ps.CustomLibrary(\n",
    "                library_functions=custom_lib_functions,\n",
    "                function_names=custom_lib_names,\n",
    "                include_bias=True\n",
    "            )\n",
    "        elif self.library == 'poly_lib':\n",
    "            library_datasindy = ps.PolynomialLibrary(poly_order)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported library type\")\n",
    "\n",
    "        if self.dataset_type == 'synt':\n",
    "            x_train, control, feature_names = make_sindy_data(experiment_list_train, agent, get_choices=self.get_choices)\n",
    "\n",
    "            optimizer = ps.STLSQ(threshold=self.threshold, verbose=True, alpha=0.1)\n",
    "            datasindy = ps.SINDy(\n",
    "                optimizer=optimizer,\n",
    "                feature_library=library_datasindy,\n",
    "                discrete_time=True,\n",
    "                feature_names=feature_names\n",
    "            )\n",
    "            datasindy.fit(x_train, t=self.dt, u=control, ensemble=self.ensemble, library_ensemble=self.library_ensemble, multiple_trajectories=True)\n",
    "            datasindy.print()\n",
    "\n",
    "            return datasindy\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dataset type\")\n",
    "        \n",
    "    def update_rule(self, datasindy, get_choices):\n",
    "        if not get_choices:\n",
    "            return lambda q, choice, reward: datasindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n",
    "        else:\n",
    "            return lambda q, choice, reward: datasindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SINDyTrainerGroundTruth(library='poly_lib')\n",
    "rnnsindyagent = AgentSindy(alpha=0, beta=1, n_actions=2)\n",
    "rnnsindyagent.set_update_rule(trainer.update_rule)\n",
    "trainer.fit(experiment_list_train, agent=agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN agent for Sindy\n",
    "# rnn_params --- from RNN training \n",
    "\n",
    "hybrnn_agent = AgentNetwork_VisibleState(rnn.make_hybrnn(), rnn_params, habit=habit_weight==1, n_actions=n_actions)\n",
    "\n",
    "dataset_hybrnn, experiment_list_hybrnn = bandits.create_dataset(hybrnn_agent, environment, n_trials_per_session, int(n_sessions*1e0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labrotation-daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
