{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from typing import Callable, Tuple, Iterable, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "\n",
    "# deepmind related libraries\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import pysindy as ps\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import bandits, disrnn, hybrnn, hybrnn_forget, plotting, rat_data, rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title make update rule of Q-/SINDyNetwork-Agents adjustable and make values of RNN-Agent visible\n",
    "\n",
    "class AgentQuadQ(bandits.AgentQ):\n",
    "  \n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perseveration_bias: float=0.,\n",
    "      ):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "  \n",
    "  def update(self,\n",
    "            choice: int,\n",
    "            reward: float):\n",
    "    \"\"\"Update the agent after one step of the task.\n",
    "\n",
    "    Args:\n",
    "      choice: The choice made by the agent. 0 or 1\n",
    "      reward: The reward received by the agent. 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decay q-values toward the initial value.\n",
    "    self._q = (1-self._forgetting_rate) * self._q + self._forgetting_rate * self._q_init\n",
    "\n",
    "    # Update chosen q for chosen action with observed reward.\n",
    "    self._q[choice] = self._q[choice] - self._alpha * self._q[choice]**2 + self._alpha * reward\n",
    "\n",
    "\n",
    "class AgentSindy(bandits.AgentQ):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perservation_bias: float=0.,):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perservation_bias)\n",
    "\n",
    "    self._update_rule = lambda q, choice, reward: (1 - self._alpha) * q[choice] + self._alpha * reward\n",
    "    self._update_rule_formula = None\n",
    "\n",
    "  def set_update_rule(self, update_rule: callable, update_rule_formula: str=None):\n",
    "    self._update_rule=update_rule\n",
    "    self._update_rule_formula=update_rule_formula\n",
    "\n",
    "  @property\n",
    "  def update_rule(self):\n",
    "    if self._update_rule_formula is not None:\n",
    "      return self._update_rule_formula\n",
    "    else:\n",
    "      return f'{self._update_rule}'\n",
    "\n",
    "  def update(self, choice: int, reward: int):\n",
    "\n",
    "    for c in range(self._n_actions):\n",
    "      self._q[c] = self._update_rule(self._q[c], int(c==choice), reward)\n",
    "\n",
    "\n",
    "class AgentNetwork_VisibleState(bandits.AgentNetwork):\n",
    "\n",
    "  def __init__(self,\n",
    "               make_network: Callable[[], hk.RNNCore],\n",
    "               params: hk.Params,\n",
    "               n_actions: int = 2,\n",
    "               state_to_numpy: bool = False,\n",
    "               habit=False):\n",
    "    super().__init__(make_network=make_network, params=params, n_actions=n_actions, state_to_numpy=state_to_numpy)\n",
    "    self.habit = habit\n",
    "\n",
    "  @property\n",
    "  def q(self):\n",
    "    if self.habit:\n",
    "      return self._state[2], self._state[3]\n",
    "    else:\n",
    "      return self._state[3].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_agents = {\n",
    "    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n",
    "    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    def __init__(self, dataset_type, agent_dict):\n",
    "        self.dataset_type = dataset_type\n",
    "        #self.environment = None\n",
    "        self.agent_dict = agent_dict\n",
    "\n",
    "    def create_dataset(self):\n",
    "        if self.dataset_type == 'synt':\n",
    "            self.setup_synthetic_data()\n",
    "            self.dataset_train, self.experiment_list_train = self.generate_data()\n",
    "            self.dataset_test, self.experiment_list_test = self.generate_data()\n",
    "        \n",
    "        elif self.dataset_type == 'real':\n",
    "            raise NotImplementedError('Real data setup not implemented yet.')\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f'dataset_type {self.dataset_type} not implemented. Please select from drop-down list.')\n",
    "\n",
    "    def setup_synthetic_data(self):\n",
    "        # Define agent parameters\n",
    "        agent_kw = 'basic'  # ['basic', 'quad_q']\n",
    "        gen_alpha = 0.25\n",
    "        gen_beta = 5\n",
    "        forgetting_rate = 0.1\n",
    "        perseveration_bias = 0.0\n",
    "        \n",
    "        # Define environment parameters\n",
    "        non_binary_reward = False\n",
    "        n_actions = 2\n",
    "        sigma = 0.1\n",
    "        \n",
    "        # Define experiment parameters\n",
    "        self.n_trials_per_session = 200\n",
    "        self.n_sessions = 220\n",
    "        \n",
    "        # Setup environment and agent\n",
    "        self.environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n",
    "        self.agent = self.agent_dict[agent_kw](gen_alpha, gen_beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "    \n",
    "    def setup_real_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        return bandits.create_dataset(\n",
    "            agent=self.agent,\n",
    "            environment=self.environment,\n",
    "            n_trials_per_session=self.n_trials_per_session,\n",
    "            n_sessions=self.n_sessions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetCreator(dataset_type='synt', agent_dict=dict_agents)\n",
    "data.create_dataset()\n",
    "\n",
    "dataset_train, experiment_list_train = data.dataset_train, data.experiment_list_train\n",
    "dataset_test, experiment_list_test = data.dataset_test, data.experiment_list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRNN:\n",
    "    def __init__(self, use_hidden_state=False, use_previous_values=False, fit_forget=False, habit_weight=0.0, value_weight=1.0, n_actions=2, hidden_size=16):\n",
    "        # Store parameters\n",
    "        self.use_hidden_state = use_hidden_state\n",
    "        self.use_previous_values = use_previous_values\n",
    "        self.fit_forget = fit_forget\n",
    "        self.habit_weight = float(habit_weight)\n",
    "        self.value_weight = value_weight\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Set up the RNN parameters\n",
    "        self.rnn_rl_params = {\n",
    "            's': self.use_hidden_state,\n",
    "            'o': self.use_previous_values,\n",
    "            'fit_forget': self.fit_forget,\n",
    "            'forget': 0.,\n",
    "            'w_h': self.habit_weight,\n",
    "            'w_v': self.value_weight\n",
    "        }\n",
    "        self.network_params = {\n",
    "            'n_actions': self.n_actions,\n",
    "            'hidden_size': self.hidden_size\n",
    "        }\n",
    "        \n",
    "        # Initialize the model\n",
    "        # self.model = self.make_hybrnn()\n",
    "        \n",
    "    \n",
    "    def make_hybrnn(self):\n",
    "        return hybrnn_forget.BiRNN(rl_params=self.rnn_rl_params, network_params=self.network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `hk.Module`s must be initialized inside an `hk.transform`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create a Hybrid RNN model instance with default parameters\u001b[39;00m\n\u001b[1;32m      3\u001b[0m hybrid_rnn \u001b[38;5;241m=\u001b[39m HybridRNN()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mhybrid_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_hybrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m optimizer_rnn \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m, in \u001b[0;36mHybridRNN.make_hybrnn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_hybrnn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhybrnn_forget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBiRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrl_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_rl_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/labrotation-daniel/lib/python3.11/site-packages/haiku/_src/module.py:151\u001b[0m, in \u001b[0;36mModuleMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Now attempt to initialize the object.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m init \u001b[38;5;241m=\u001b[39m wrap_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m ran_super_ctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ran_super_ctor:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/labrotation-daniel/lib/python3.11/site-packages/haiku/_src/module.py:439\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the original method with a group name set before and after.\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m base\u001b[38;5;241m.\u001b[39mframe_stack:\n\u001b[0;32m--> 439\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    440\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `hk.Module`s must be initialized inside an `hk.transform`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Submodules are associated with this method. We allow users to associate\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# submodules with a different method than the one being called via\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# `@name_like(\"other_method\")`. Interceptors and custom getters are still\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# provided the actual method name (e.g. \"submodule_method_name\" is only used\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# for naming submodules).\u001b[39;00m\n\u001b[1;32m    447\u001b[0m submodule_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(unbound_method, _CUSTOM_NAME, method_name)\n",
      "\u001b[0;31mValueError\u001b[0m: All `hk.Module`s must be initialized inside an `hk.transform`."
     ]
    }
   ],
   "source": [
    "# Create a Hybrid RNN model\n",
    "hybrid_rnn = HybridRNN()\n",
    "hybrid_rnn.make_hybrnn()\n",
    "optimizer_rnn = optax.adam(learning_rate=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labrotation-daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
