{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import rnn, rnn_training, bandits, rnn_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3.pkl.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train model\n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "use_lstm = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_habit = False\n",
    "\n",
    "# ensemble parameters\n",
    "sampling_replacement = True\n",
    "n_submodels = 1   # baseline model / no ensembling\n",
    "ensemble = False\n",
    "voting_type = rnn.EnsembleRNN.MEAN  # necessary if ensemble==True, can be mean or median\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "if use_habit:\n",
    "  x_train_list += ['xHf']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "# training parameters\n",
    "epochs = 8   # change to 100 (madd)\n",
    "n_steps_per_call = 10  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not data:\n",
    "  # agent parameters\n",
    "  agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n",
    "  gen_alpha = .25 #@param\n",
    "  gen_beta = 3 #@param\n",
    "  forget_rate = 0. #@param\n",
    "  perseverance_bias = 0. #@param\n",
    "  # environment parameters\n",
    "  non_binary_reward = False #@param\n",
    "  n_actions = 2 #@param\n",
    "  sigma = .1  #@param\n",
    "\n",
    "  # dataset parameters\n",
    "  n_trials_per_session = 200  #@param\n",
    "  n_sessions = 64  #@param\n",
    "\n",
    "\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,  #n_trials_per_session,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      use_habit,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      use_habit=use_habit,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "\n",
    "optimizer_rnn = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the hybrid RNN...\n",
      "Epoch 1/8 --- Loss: 0.6646731; Time: 0.5s; Convergence value: 3.35e-01\n",
      "Epoch 2/8 --- Loss: 0.6363984; Time: 0.4s; Convergence value: 1.75e-01\n",
      "Epoch 3/8 --- Loss: 0.6075963; Time: 0.4s; Convergence value: 1.22e-01\n",
      "Epoch 4/8 --- Loss: 0.5902762; Time: 0.4s; Convergence value: 9.11e-02\n",
      "Epoch 5/8 --- Loss: 0.5812210; Time: 0.4s; Convergence value: 7.05e-02\n",
      "Epoch 6/8 --- Loss: 0.5499825; Time: 0.5s; Convergence value: 6.11e-02\n",
      "Epoch 7/8 --- Loss: 0.5548703; Time: 0.4s; Convergence value: 4.90e-02\n",
      "Epoch 8/8 --- Loss: 0.5572730; Time: 0.7s; Convergence value: 1.57e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n"
     ]
    }
   ],
   "source": [
    "from resources import rnn, rnn_training, bandits, rnn_utils\n",
    "\n",
    "if train:\n",
    "  if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    optimizer_rnn.load_state_dict(state_dict['optimizer'])\n",
    "    print('Loaded parameters.')\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, training_loss = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      n_steps_per_call = n_steps_per_call,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      return_ensemble=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "  )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5315, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "  with torch.no_grad():\n",
    "    model, optimizer_rnn, val_loss, losses_list = rnn_training.fit_model(     #val_loss gets the final validation loss\n",
    "        model=model,\n",
    "        dataset=dataset_test,\n",
    "        n_steps_per_call=1,\n",
    "    )\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "  \n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "\n",
    "else:\n",
    "  # load trained parameters\n",
    "  model.load_state_dict(torch.load(params_path)['model'])\n",
    "  print(f'Loaded parameters from file {params_path}.')\n",
    "\n",
    "# if hasattr(model, 'beta'):\n",
    "#   print(f'beta: {model.beta}')\n",
    "\n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2, habit=use_habit)\n",
    "# dataset_rnn, experiment_list_rnn = bandits.create_dataset(rnn_agent, environment, 220, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-3aa359b3f5f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(float(v_loss))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(float(v_loss))\n",
    "print(rnn_training.fit_model.loss)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analysis\n",
    "session_id = 0\n",
    "\n",
    "choices = experiment_list_test[session_id].choices\n",
    "rewards = experiment_list_test[session_id].rewards\n",
    "\n",
    "list_probs = []\n",
    "list_qs = []\n",
    "\n",
    "# get q-values from groundtruth\n",
    "qs_test, probs_test = bandits.get_update_dynamics(experiment_list_test[session_id], agent)\n",
    "list_probs.append(np.expand_dims(probs_test, 0))\n",
    "list_qs.append(np.expand_dims(qs_test, 0))\n",
    "\n",
    "# get q-values from trained rnn\n",
    "qs_rnn, probs_rnn = bandits.get_update_dynamics(experiment_list_test[session_id], rnn_agent)\n",
    "list_probs.append(np.expand_dims(probs_rnn, 0))\n",
    "list_qs.append(np.expand_dims(qs_rnn, 0))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:grey']\n",
    "\n",
    "# concatenate all choice probs and q-values\n",
    "probs = np.concatenate(list_probs, axis=0)\n",
    "qs = np.concatenate(list_qs, axis=0)\n",
    "\n",
    "# normalize q-values\n",
    "# qs = (qs - np.min(qs, axis=1, keepdims=True)) / (np.max(qs, axis=1, keepdims=True) - np.min(qs, axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: 2 submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/100 --- Loss: 0.6391087; Time: 1.0s; Convergence value: 3.61e-01\n",
      "Epoch 2/100 --- Loss: 0.5916474; Time: 1.0s; Convergence value: 2.02e-01\n",
      "Epoch 3/100 --- Loss: 0.5740830; Time: 1.2s; Convergence value: 1.39e-01\n",
      "Epoch 4/100 --- Loss: 0.5851295; Time: 1.1s; Convergence value: 1.05e-01\n",
      "Epoch 5/100 --- Loss: 0.5633962; Time: 1.2s; Convergence value: 8.75e-02\n",
      "Epoch 6/100 --- Loss: 0.5581319; Time: 1.1s; Convergence value: 7.27e-02\n",
      "Epoch 7/100 --- Loss: 0.5494964; Time: 1.2s; Convergence value: 6.26e-02\n",
      "Epoch 8/100 --- Loss: 0.5561534; Time: 1.0s; Convergence value: 5.47e-02\n",
      "Epoch 9/100 --- Loss: 0.5690601; Time: 1.1s; Convergence value: 4.93e-02\n",
      "Epoch 10/100 --- Loss: 0.5492237; Time: 0.9s; Convergence value: 4.58e-02\n",
      "Epoch 11/100 --- Loss: 0.5367042; Time: 0.9s; Convergence value: 4.21e-02\n",
      "Epoch 12/100 --- Loss: 0.5391178; Time: 1.0s; Convergence value: 3.81e-02\n",
      "Epoch 13/100 --- Loss: 0.5549519; Time: 0.8s; Convergence value: 3.58e-02\n",
      "Epoch 14/100 --- Loss: 0.5439250; Time: 0.7s; Convergence value: 3.35e-02\n",
      "Epoch 15/100 --- Loss: 0.5654765; Time: 0.8s; Convergence value: 3.23e-02\n",
      "Epoch 16/100 --- Loss: 0.5597517; Time: 0.7s; Convergence value: 3.00e-02\n",
      "Epoch 17/100 --- Loss: 0.5432351; Time: 0.7s; Convergence value: 2.88e-02\n",
      "Epoch 18/100 --- Loss: 0.5333968; Time: 0.7s; Convergence value: 2.72e-02\n",
      "Epoch 19/100 --- Loss: 0.5379020; Time: 0.9s; Convergence value: 2.54e-02\n",
      "Epoch 20/100 --- Loss: 0.5396498; Time: 1.0s; Convergence value: 1.23e-02\n",
      "Epoch 21/100 --- Loss: 0.5482937; Time: 0.8s; Convergence value: 1.09e-02\n",
      "Epoch 22/100 --- Loss: 0.5283580; Time: 0.8s; Convergence value: 1.13e-02\n",
      "Epoch 23/100 --- Loss: 0.5446664; Time: 0.8s; Convergence value: 1.16e-02\n",
      "Epoch 24/100 --- Loss: 0.5349727; Time: 0.8s; Convergence value: 1.12e-02\n",
      "Epoch 25/100 --- Loss: 0.5322025; Time: 1.0s; Convergence value: 1.08e-02\n",
      "Epoch 26/100 --- Loss: 0.5467862; Time: 0.9s; Convergence value: 1.11e-02\n",
      "Epoch 27/100 --- Loss: 0.5283412; Time: 0.8s; Convergence value: 1.18e-02\n",
      "Epoch 28/100 --- Loss: 0.5346843; Time: 0.9s; Convergence value: 1.13e-02\n",
      "Epoch 29/100 --- Loss: 0.5667458; Time: 0.8s; Convergence value: 1.25e-02\n",
      "Epoch 30/100 --- Loss: 0.5164483; Time: 0.7s; Convergence value: 1.52e-02\n",
      "Epoch 31/100 --- Loss: 0.5279828; Time: 0.7s; Convergence value: 1.54e-02\n",
      "Epoch 32/100 --- Loss: 0.5579982; Time: 0.8s; Convergence value: 1.64e-02\n",
      "Epoch 33/100 --- Loss: 0.5563750; Time: 0.8s; Convergence value: 1.56e-02\n",
      "Epoch 34/100 --- Loss: 0.5421007; Time: 0.7s; Convergence value: 1.53e-02\n",
      "Epoch 35/100 --- Loss: 0.5561733; Time: 0.9s; Convergence value: 1.56e-02\n",
      "Epoch 36/100 --- Loss: 0.5427815; Time: 0.8s; Convergence value: 1.54e-02\n",
      "Epoch 37/100 --- Loss: 0.5318732; Time: 0.8s; Convergence value: 1.53e-02\n",
      "Epoch 38/100 --- Loss: 0.5293298; Time: 0.8s; Convergence value: 1.48e-02\n",
      "Epoch 39/100 --- Loss: 0.5531710; Time: 0.8s; Convergence value: 1.59e-02\n",
      "Epoch 40/100 --- Loss: 0.5297928; Time: 0.8s; Convergence value: 1.67e-02\n",
      "Epoch 41/100 --- Loss: 0.5279200; Time: 0.8s; Convergence value: 1.55e-02\n",
      "Epoch 42/100 --- Loss: 0.5326977; Time: 0.8s; Convergence value: 1.47e-02\n",
      "Epoch 43/100 --- Loss: 0.5532559; Time: 0.8s; Convergence value: 1.53e-02\n",
      "Epoch 44/100 --- Loss: 0.5350060; Time: 0.8s; Convergence value: 1.59e-02\n",
      "Epoch 45/100 --- Loss: 0.5592684; Time: 0.7s; Convergence value: 1.65e-02\n",
      "Epoch 46/100 --- Loss: 0.5203661; Time: 0.9s; Convergence value: 1.80e-02\n",
      "Epoch 47/100 --- Loss: 0.5301861; Time: 0.7s; Convergence value: 1.78e-02\n",
      "Epoch 48/100 --- Loss: 0.5550981; Time: 0.7s; Convergence value: 1.78e-02\n",
      "Epoch 49/100 --- Loss: 0.5432888; Time: 0.7s; Convergence value: 1.63e-02\n",
      "Epoch 50/100 --- Loss: 0.5571700; Time: 1.0s; Convergence value: 1.63e-02\n",
      "Epoch 51/100 --- Loss: 0.5586888; Time: 0.7s; Convergence value: 1.48e-02\n",
      "Epoch 52/100 --- Loss: 0.5503066; Time: 0.7s; Convergence value: 1.48e-02\n",
      "Epoch 53/100 --- Loss: 0.5407982; Time: 0.8s; Convergence value: 1.45e-02\n",
      "Epoch 54/100 --- Loss: 0.5018409; Time: 0.7s; Convergence value: 1.62e-02\n",
      "Epoch 55/100 --- Loss: 0.5362822; Time: 1.0s; Convergence value: 1.76e-02\n",
      "Epoch 56/100 --- Loss: 0.5534136; Time: 1.0s; Convergence value: 1.78e-02\n",
      "Epoch 57/100 --- Loss: 0.5396954; Time: 0.8s; Convergence value: 1.80e-02\n",
      "Epoch 58/100 --- Loss: 0.5407884; Time: 0.8s; Convergence value: 1.67e-02\n",
      "Epoch 59/100 --- Loss: 0.5355543; Time: 0.8s; Convergence value: 1.56e-02\n",
      "Epoch 60/100 --- Loss: 0.5451194; Time: 0.9s; Convergence value: 1.57e-02\n",
      "Epoch 61/100 --- Loss: 0.5317857; Time: 0.7s; Convergence value: 1.59e-02\n",
      "Epoch 62/100 --- Loss: 0.5310112; Time: 0.7s; Convergence value: 1.46e-02\n",
      "Epoch 63/100 --- Loss: 0.5221090; Time: 0.8s; Convergence value: 1.41e-02\n",
      "Epoch 64/100 --- Loss: 0.5690261; Time: 0.7s; Convergence value: 1.60e-02\n",
      "Epoch 65/100 --- Loss: 0.5359765; Time: 0.7s; Convergence value: 1.64e-02\n",
      "Epoch 66/100 --- Loss: 0.5848958; Time: 0.8s; Convergence value: 1.89e-02\n",
      "Epoch 67/100 --- Loss: 0.5199118; Time: 0.9s; Convergence value: 2.20e-02\n",
      "Epoch 68/100 --- Loss: 0.5211086; Time: 0.8s; Convergence value: 2.09e-02\n",
      "Epoch 69/100 --- Loss: 0.5501173; Time: 0.7s; Convergence value: 2.18e-02\n",
      "Epoch 70/100 --- Loss: 0.5592182; Time: 0.7s; Convergence value: 2.16e-02\n",
      "Epoch 71/100 --- Loss: 0.5438784; Time: 0.9s; Convergence value: 2.17e-02\n",
      "Epoch 72/100 --- Loss: 0.5381602; Time: 0.9s; Convergence value: 2.10e-02\n",
      "Epoch 73/100 --- Loss: 0.5618383; Time: 0.7s; Convergence value: 2.06e-02\n",
      "Epoch 74/100 --- Loss: 0.5296338; Time: 0.6s; Convergence value: 2.09e-02\n",
      "Epoch 75/100 --- Loss: 0.5477521; Time: 0.7s; Convergence value: 2.09e-02\n",
      "Epoch 76/100 --- Loss: 0.5371115; Time: 0.8s; Convergence value: 2.04e-02\n",
      "Epoch 77/100 --- Loss: 0.5428026; Time: 0.8s; Convergence value: 2.01e-02\n",
      "Epoch 78/100 --- Loss: 0.5340699; Time: 0.7s; Convergence value: 1.98e-02\n",
      "Epoch 79/100 --- Loss: 0.5257015; Time: 0.7s; Convergence value: 1.93e-02\n",
      "Epoch 80/100 --- Loss: 0.5672295; Time: 0.8s; Convergence value: 2.10e-02\n",
      "Epoch 81/100 --- Loss: 0.5443739; Time: 0.9s; Convergence value: 2.18e-02\n",
      "Epoch 82/100 --- Loss: 0.5719450; Time: 0.8s; Convergence value: 2.26e-02\n",
      "Epoch 83/100 --- Loss: 0.5315994; Time: 1.0s; Convergence value: 2.30e-02\n",
      "Epoch 84/100 --- Loss: 0.5276613; Time: 0.8s; Convergence value: 2.13e-02\n",
      "Epoch 85/100 --- Loss: 0.4975469; Time: 0.9s; Convergence value: 2.10e-02\n",
      "Epoch 86/100 --- Loss: 0.5600816; Time: 0.8s; Convergence value: 2.24e-02\n",
      "Epoch 87/100 --- Loss: 0.5615646; Time: 0.7s; Convergence value: 2.17e-02\n",
      "Epoch 88/100 --- Loss: 0.5098224; Time: 0.8s; Convergence value: 2.36e-02\n",
      "Epoch 89/100 --- Loss: 0.5293559; Time: 0.8s; Convergence value: 2.39e-02\n",
      "Epoch 90/100 --- Loss: 0.5268455; Time: 0.8s; Convergence value: 2.27e-02\n",
      "Epoch 91/100 --- Loss: 0.5473220; Time: 0.9s; Convergence value: 2.31e-02\n",
      "Epoch 92/100 --- Loss: 0.5351552; Time: 0.9s; Convergence value: 2.23e-02\n",
      "Epoch 93/100 --- Loss: 0.5482925; Time: 1.0s; Convergence value: 2.14e-02\n",
      "Epoch 94/100 --- Loss: 0.5158710; Time: 0.8s; Convergence value: 2.23e-02\n",
      "Epoch 95/100 --- Loss: 0.5198319; Time: 1.1s; Convergence value: 2.14e-02\n",
      "Epoch 96/100 --- Loss: 0.5188533; Time: 0.8s; Convergence value: 2.05e-02\n",
      "Epoch 97/100 --- Loss: 0.5274826; Time: 0.8s; Convergence value: 2.00e-02\n",
      "Epoch 98/100 --- Loss: 0.5263320; Time: 0.9s; Convergence value: 1.90e-02\n",
      "Epoch 99/100 --- Loss: 0.5183117; Time: 0.8s; Convergence value: 1.74e-02\n",
      "Epoch 100/100 --- Loss: 0.5360753; Time: 0.7s; Convergence value: 1.72e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5257104; Time: 1.4s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 85.05 seconds.\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "use_lstm = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_habit = False\n",
    "\n",
    "# ensemble parameters\n",
    "sampling_replacement = True\n",
    "n_submodels = 2   \n",
    "ensemble = False\n",
    "voting_type = rnn.EnsembleRNN.MEAN  # necessary if ensemble==True, can be mean or median\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "if use_habit:\n",
    "  x_train_list += ['xHf']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "# training parameters\n",
    "epochs = 100\n",
    "n_steps_per_call = 10  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not data:\n",
    "  # agent parameters\n",
    "  agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n",
    "  gen_alpha = .25 #@param\n",
    "  gen_beta = 3 #@param\n",
    "  forget_rate = 0. #@param\n",
    "  perseverance_bias = 0. #@param\n",
    "  # environment parameters\n",
    "  non_binary_reward = False #@param\n",
    "  n_actions = 2 #@param\n",
    "  sigma = .1  #@param\n",
    "\n",
    "  # dataset parameters\n",
    "  n_trials_per_session = 200  #@param\n",
    "  n_sessions = 64  #@param\n",
    "\n",
    "\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,#n_trials_per_session,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      use_habit,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      use_habit=use_habit,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "\n",
    "optimizer_rnn = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if train:\n",
    "  if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    optimizer_rnn.load_state_dict(state_dict['optimizer'])\n",
    "    print('Loaded parameters.')\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      n_steps_per_call = n_steps_per_call,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      return_ensemble=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "  )\n",
    "  \n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "  with torch.no_grad():\n",
    "    model, optimizer_rnn, val_loss = rnn_training.fit_model(     #val_loss gets the final validation loss\n",
    "        model=model,\n",
    "        dataset=dataset_test,\n",
    "        n_steps_per_call=1,\n",
    "    )\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "  \n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "\n",
    "else:\n",
    "  # load trained parameters\n",
    "  model.load_state_dict(torch.load(params_path)['model'])\n",
    "  print(f'Loaded parameters from file {params_path}.')\n",
    "\n",
    "# if hasattr(model, 'beta'):\n",
    "#   print(f'beta: {model.beta}')\n",
    "\n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2, habit=use_habit)\n",
    "# dataset_rnn, experiment_list_rnn = bandits.create_dataset(rnn_agent, environment, 220, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/100 --- Loss: 0.6557027; Time: 1.5s; Convergence value: 3.44e-01\n",
      "Epoch 2/100 --- Loss: 0.6089835; Time: 1.7s; Convergence value: 1.93e-01\n",
      "Epoch 3/100 --- Loss: 0.5718898; Time: 1.6s; Convergence value: 1.40e-01\n",
      "Epoch 4/100 --- Loss: 0.5655484; Time: 1.7s; Convergence value: 1.05e-01\n",
      "Epoch 5/100 --- Loss: 0.5846874; Time: 1.5s; Convergence value: 8.67e-02\n",
      "Epoch 6/100 --- Loss: 0.5786796; Time: 1.5s; Convergence value: 7.21e-02\n",
      "Epoch 7/100 --- Loss: 0.5775130; Time: 1.4s; Convergence value: 6.09e-02\n",
      "Epoch 8/100 --- Loss: 0.5572833; Time: 1.1s; Convergence value: 5.52e-02\n",
      "Epoch 9/100 --- Loss: 0.5614052; Time: 1.2s; Convergence value: 4.87e-02\n",
      "Epoch 10/100 --- Loss: 0.5979993; Time: 1.2s; Convergence value: 4.71e-02\n",
      "Epoch 11/100 --- Loss: 0.5673509; Time: 1.3s; Convergence value: 4.52e-02\n",
      "Epoch 12/100 --- Loss: 0.5617303; Time: 1.3s; Convergence value: 4.12e-02\n",
      "Epoch 13/100 --- Loss: 0.5636719; Time: 1.1s; Convergence value: 3.74e-02\n",
      "Epoch 14/100 --- Loss: 0.5714917; Time: 1.1s; Convergence value: 3.46e-02\n",
      "Epoch 15/100 --- Loss: 0.5823465; Time: 1.3s; Convergence value: 3.25e-02\n",
      "Epoch 16/100 --- Loss: 0.5710803; Time: 1.3s; Convergence value: 3.06e-02\n",
      "Epoch 17/100 --- Loss: 0.5772173; Time: 1.6s; Convergence value: 2.86e-02\n",
      "Epoch 18/100 --- Loss: 0.5777040; Time: 1.1s; Convergence value: 2.63e-02\n",
      "Epoch 19/100 --- Loss: 0.5389336; Time: 1.1s; Convergence value: 2.69e-02\n",
      "Epoch 20/100 --- Loss: 0.5735148; Time: 1.1s; Convergence value: 1.67e-02\n",
      "Epoch 21/100 --- Loss: 0.5326564; Time: 1.1s; Convergence value: 1.74e-02\n",
      "Epoch 22/100 --- Loss: 0.5693721; Time: 1.5s; Convergence value: 1.81e-02\n",
      "Epoch 23/100 --- Loss: 0.5475819; Time: 1.5s; Convergence value: 1.88e-02\n",
      "Epoch 24/100 --- Loss: 0.5543875; Time: 1.4s; Convergence value: 1.80e-02\n",
      "Epoch 25/100 --- Loss: 0.5793723; Time: 1.5s; Convergence value: 1.89e-02\n",
      "Epoch 26/100 --- Loss: 0.5480953; Time: 1.7s; Convergence value: 2.04e-02\n",
      "Epoch 27/100 --- Loss: 0.5793865; Time: 1.8s; Convergence value: 2.12e-02\n",
      "Epoch 28/100 --- Loss: 0.5602775; Time: 1.5s; Convergence value: 2.17e-02\n",
      "Epoch 29/100 --- Loss: 0.5789490; Time: 1.6s; Convergence value: 2.10e-02\n",
      "Epoch 30/100 --- Loss: 0.5639251; Time: 1.3s; Convergence value: 2.03e-02\n",
      "Epoch 31/100 --- Loss: 0.5686466; Time: 1.5s; Convergence value: 1.97e-02\n",
      "Epoch 32/100 --- Loss: 0.5366148; Time: 1.5s; Convergence value: 2.12e-02\n",
      "Epoch 33/100 --- Loss: 0.5612122; Time: 1.7s; Convergence value: 2.19e-02\n",
      "Epoch 34/100 --- Loss: 0.5857391; Time: 1.9s; Convergence value: 2.25e-02\n",
      "Epoch 35/100 --- Loss: 0.5800636; Time: 1.5s; Convergence value: 2.17e-02\n",
      "Epoch 36/100 --- Loss: 0.5544711; Time: 1.6s; Convergence value: 2.24e-02\n",
      "Epoch 37/100 --- Loss: 0.5675852; Time: 1.0s; Convergence value: 2.25e-02\n",
      "Epoch 38/100 --- Loss: 0.5624918; Time: 1.3s; Convergence value: 2.07e-02\n",
      "Epoch 39/100 --- Loss: 0.5640003; Time: 1.1s; Convergence value: 1.88e-02\n",
      "Epoch 40/100 --- Loss: 0.5644289; Time: 1.1s; Convergence value: 1.68e-02\n",
      "Epoch 41/100 --- Loss: 0.5543544; Time: 1.1s; Convergence value: 1.56e-02\n",
      "Epoch 42/100 --- Loss: 0.5708936; Time: 1.2s; Convergence value: 1.54e-02\n",
      "Epoch 43/100 --- Loss: 0.5514137; Time: 1.3s; Convergence value: 1.59e-02\n",
      "Epoch 44/100 --- Loss: 0.5442360; Time: 1.5s; Convergence value: 1.50e-02\n",
      "Epoch 45/100 --- Loss: 0.5834588; Time: 1.3s; Convergence value: 1.61e-02\n",
      "Epoch 46/100 --- Loss: 0.5850208; Time: 1.4s; Convergence value: 1.46e-02\n",
      "Epoch 47/100 --- Loss: 0.5634628; Time: 1.4s; Convergence value: 1.49e-02\n",
      "Epoch 48/100 --- Loss: 0.5525467; Time: 1.4s; Convergence value: 1.45e-02\n",
      "Epoch 49/100 --- Loss: 0.5696986; Time: 1.6s; Convergence value: 1.46e-02\n",
      "Epoch 50/100 --- Loss: 0.5775149; Time: 2.5s; Convergence value: 1.45e-02\n",
      "Epoch 51/100 --- Loss: 0.5525100; Time: 2.3s; Convergence value: 1.46e-02\n",
      "Epoch 52/100 --- Loss: 0.5738010; Time: 1.6s; Convergence value: 1.48e-02\n",
      "Epoch 53/100 --- Loss: 0.5637111; Time: 1.6s; Convergence value: 1.41e-02\n",
      "Epoch 54/100 --- Loss: 0.5780037; Time: 1.9s; Convergence value: 1.44e-02\n",
      "Epoch 55/100 --- Loss: 0.5746631; Time: 1.3s; Convergence value: 1.33e-02\n",
      "Epoch 56/100 --- Loss: 0.5355031; Time: 1.1s; Convergence value: 1.51e-02\n",
      "Epoch 57/100 --- Loss: 0.5836496; Time: 1.3s; Convergence value: 1.78e-02\n",
      "Epoch 58/100 --- Loss: 0.5261514; Time: 1.2s; Convergence value: 2.12e-02\n",
      "Epoch 59/100 --- Loss: 0.5487376; Time: 2.0s; Convergence value: 2.20e-02\n",
      "Epoch 60/100 --- Loss: 0.5559027; Time: 1.4s; Convergence value: 2.14e-02\n",
      "Epoch 61/100 --- Loss: 0.5726891; Time: 1.0s; Convergence value: 2.13e-02\n",
      "Epoch 62/100 --- Loss: 0.5658826; Time: 1.1s; Convergence value: 2.04e-02\n",
      "Epoch 63/100 --- Loss: 0.5476850; Time: 1.1s; Convergence value: 2.07e-02\n",
      "Epoch 64/100 --- Loss: 0.5528045; Time: 1.0s; Convergence value: 1.90e-02\n",
      "Epoch 65/100 --- Loss: 0.5706578; Time: 1.0s; Convergence value: 1.95e-02\n",
      "Epoch 66/100 --- Loss: 0.5396014; Time: 1.0s; Convergence value: 2.02e-02\n",
      "Epoch 67/100 --- Loss: 0.5484043; Time: 1.2s; Convergence value: 1.98e-02\n",
      "Epoch 68/100 --- Loss: 0.5975550; Time: 1.2s; Convergence value: 2.19e-02\n",
      "Epoch 69/100 --- Loss: 0.5749469; Time: 1.0s; Convergence value: 2.24e-02\n",
      "Epoch 70/100 --- Loss: 0.5499553; Time: 1.0s; Convergence value: 2.25e-02\n",
      "Epoch 71/100 --- Loss: 0.5556050; Time: 1.1s; Convergence value: 2.14e-02\n",
      "Epoch 72/100 --- Loss: 0.5466291; Time: 1.0s; Convergence value: 2.09e-02\n",
      "Epoch 73/100 --- Loss: 0.5553355; Time: 1.1s; Convergence value: 2.02e-02\n",
      "Epoch 74/100 --- Loss: 0.5679750; Time: 1.2s; Convergence value: 2.02e-02\n",
      "Epoch 75/100 --- Loss: 0.5692205; Time: 1.2s; Convergence value: 1.82e-02\n",
      "Epoch 76/100 --- Loss: 0.5971537; Time: 1.1s; Convergence value: 1.78e-02\n",
      "Epoch 77/100 --- Loss: 0.5749846; Time: 1.3s; Convergence value: 1.68e-02\n",
      "Epoch 78/100 --- Loss: 0.5594050; Time: 1.1s; Convergence value: 1.65e-02\n",
      "Epoch 79/100 --- Loss: 0.5464684; Time: 1.4s; Convergence value: 1.66e-02\n",
      "Epoch 80/100 --- Loss: 0.5537667; Time: 1.4s; Convergence value: 1.59e-02\n",
      "Epoch 81/100 --- Loss: 0.5409743; Time: 1.4s; Convergence value: 1.60e-02\n",
      "Epoch 82/100 --- Loss: 0.5631945; Time: 1.3s; Convergence value: 1.63e-02\n",
      "Epoch 83/100 --- Loss: 0.5765424; Time: 1.5s; Convergence value: 1.65e-02\n",
      "Epoch 84/100 --- Loss: 0.5960581; Time: 3.2s; Convergence value: 1.66e-02\n",
      "Epoch 85/100 --- Loss: 0.5407271; Time: 2.0s; Convergence value: 1.88e-02\n",
      "Epoch 86/100 --- Loss: 0.5870313; Time: 1.7s; Convergence value: 2.11e-02\n",
      "Epoch 87/100 --- Loss: 0.5417956; Time: 1.9s; Convergence value: 2.19e-02\n",
      "Epoch 88/100 --- Loss: 0.5571482; Time: 1.5s; Convergence value: 2.15e-02\n",
      "Epoch 89/100 --- Loss: 0.5639813; Time: 1.2s; Convergence value: 2.04e-02\n",
      "Epoch 90/100 --- Loss: 0.5815094; Time: 1.2s; Convergence value: 2.07e-02\n",
      "Epoch 91/100 --- Loss: 0.5622671; Time: 1.2s; Convergence value: 2.11e-02\n",
      "Epoch 92/100 --- Loss: 0.5645370; Time: 1.5s; Convergence value: 2.02e-02\n",
      "Epoch 93/100 --- Loss: 0.5622508; Time: 1.2s; Convergence value: 1.92e-02\n",
      "Epoch 94/100 --- Loss: 0.5801729; Time: 1.3s; Convergence value: 1.97e-02\n",
      "Epoch 95/100 --- Loss: 0.5796011; Time: 1.6s; Convergence value: 1.81e-02\n",
      "Epoch 96/100 --- Loss: 0.5634424; Time: 1.5s; Convergence value: 1.78e-02\n",
      "Epoch 97/100 --- Loss: 0.5611441; Time: 1.4s; Convergence value: 1.67e-02\n",
      "Epoch 98/100 --- Loss: 0.5684580; Time: 1.3s; Convergence value: 1.62e-02\n",
      "Epoch 99/100 --- Loss: 0.5573778; Time: 1.7s; Convergence value: 1.61e-02\n",
      "Epoch 100/100 --- Loss: 0.5677468; Time: 2.4s; Convergence value: 1.57e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5050020; Time: 0.7s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a6010e12f876>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mn_steps_per_call\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     )\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "# 3 submodels\n",
    "\n",
    "# train model\n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "use_lstm = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_habit = False\n",
    "\n",
    "# ensemble parameters\n",
    "sampling_replacement = True\n",
    "n_submodels = 3   \n",
    "ensemble = False\n",
    "voting_type = rnn.EnsembleRNN.MEAN  # necessary if ensemble==True, can be mean or median\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "if use_habit:\n",
    "  x_train_list += ['xHf']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "# training parameters\n",
    "epochs = 100\n",
    "n_steps_per_call = 10  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not data:\n",
    "  # agent parameters\n",
    "  agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n",
    "  gen_alpha = .25 #@param\n",
    "  gen_beta = 3 #@param\n",
    "  forget_rate = 0. #@param\n",
    "  perseverance_bias = 0. #@param\n",
    "  # environment parameters\n",
    "  non_binary_reward = False #@param\n",
    "  n_actions = 2 #@param\n",
    "  sigma = .1  #@param\n",
    "\n",
    "  # dataset parameters\n",
    "  n_trials_per_session = 200  #@param\n",
    "  n_sessions = 64  #@param\n",
    "\n",
    "\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,#n_trials_per_session,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      use_habit,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      use_habit=use_habit,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "\n",
    "optimizer_rnn = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if train:\n",
    "  if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    optimizer_rnn.load_state_dict(state_dict['optimizer'])\n",
    "    print('Loaded parameters.')\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      n_steps_per_call = n_steps_per_call,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      return_ensemble=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "  )\n",
    "  \n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "  with torch.no_grad():\n",
    "    model, optimizer_rnn, val_loss, losses_list = rnn_training.fit_model(     #val_loss gets the final validation loss\n",
    "        model=model,\n",
    "        dataset=dataset_test,\n",
    "        n_steps_per_call=1,\n",
    "    )\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "  \n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "\n",
    "else:\n",
    "  # load trained parameters\n",
    "  model.load_state_dict(torch.load(params_path)['model'])\n",
    "  print(f'Loaded parameters from file {params_path}.')\n",
    "\n",
    "# if hasattr(model, 'beta'):\n",
    "#   print(f'beta: {model.beta}')\n",
    "\n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2, habit=use_habit)\n",
    "# dataset_rnn, experiment_list_rnn = bandits.create_dataset(rnn_agent, environment, 220, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
