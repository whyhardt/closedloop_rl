{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import rnn, rnn_training, bandits, rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b1_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6818449; Time: 11.6449s; Convergence value: 3.18e-01\n",
      "Epoch 2/5 --- Loss: 0.6814595; Time: 11.8765s; Convergence value: 1.45e-01\n",
      "Epoch 3/5 --- Loss: 0.6810587; Time: 12.0010s; Convergence value: 8.51e-02\n",
      "Epoch 4/5 --- Loss: 0.6806923; Time: 11.7729s; Convergence value: 5.33e-02\n",
      "Epoch 5/5 --- Loss: 0.6802283; Time: 11.7369s; Convergence value: 4.10e-04\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6868224; Time: 1.0394s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868225; Time: 0.7296s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868224; Time: 0.7618s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868225; Time: 0.8037s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868225; Time: 1.0472s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868225; Time: 0.7442s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868224; Time: 0.8067s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868225; Time: 0.7519s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868225; Time: 0.7234s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6868224; Time: 0.7575s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 67.24 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b1_f01.pkl.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-58652c4dcc87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;31m# get q-values from trained rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m \u001b[0mqs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_update_dynamics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_list_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msession_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[0mlist_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[0mlist_qs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\bandits.py\u001b[0m in \u001b[0;36mget_update_dynamics\u001b[1;34m(experiment, agent)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[0mqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[0mchoice_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_choice_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\bandits.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, choice, reward)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[1;31m# self.set_state(self._model.get_state(return_dict=True))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, prev_state, batch_first)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 \u001b[0maction_oh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_oh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "# train model (BASELINE)\n",
    "\n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 1\n",
    "ensemble = rnn_training.ensemble_types.NONE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 1\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  baseline_losses = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      baseline_losses.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2)\n",
    "\n",
    "# Analysis\n",
    "session_id = 0\n",
    "\n",
    "choices = experiment_list_test[session_id].choices\n",
    "rewards = experiment_list_test[session_id].rewards\n",
    "\n",
    "list_probs = []\n",
    "list_qs = []\n",
    "\n",
    "# get q-values from groundtruth\n",
    "qs_test, probs_test = bandits.get_update_dynamics(experiment_list_test[session_id], agent)\n",
    "list_probs.append(np.expand_dims(probs_test, 0))\n",
    "list_qs.append(np.expand_dims(qs_test, 0))\n",
    "\n",
    "# get q-values from trained rnn\n",
    "qs_rnn, probs_rnn = bandits.get_update_dynamics(experiment_list_test[session_id], rnn_agent)\n",
    "list_probs.append(np.expand_dims(probs_rnn, 0))\n",
    "list_qs.append(np.expand_dims(qs_rnn, 0))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:grey']\n",
    "\n",
    "# concatenate all choice probs and q-values\n",
    "probs = np.concatenate(list_probs, axis=0)\n",
    "qs = np.concatenate(list_qs, axis=0)\n",
    "\n",
    "# normalize q-values\n",
    "def normalize(qs):\n",
    "  return (qs - np.min(qs, axis=1, keepdims=True)) / (np.max(qs, axis=1, keepdims=True) - np.min(qs, axis=1, keepdims=True))\n",
    "\n",
    "qs = normalize(qs)\n",
    "fig, axs = plt.subplots(4, 1, figsize=(20, 10))\n",
    "\n",
    "reward_probs = np.stack([experiment_list_test[session_id].timeseries[:, i] for i in range(n_actions)], axis=0)\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=reward_probs,\n",
    "    timeseries_name='Reward Probs',\n",
    "    labels=[f'Arm {a}' for a in range(n_actions)],\n",
    "    color=['tab:purple', 'tab:cyan'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[0]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=probs[:, :, 0],\n",
    "    timeseries_name='Choice Probs',\n",
    "    color=colors,\n",
    "    labels=['Ground Truth', 'RNN'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[1]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=qs[:, :, 0],\n",
    "    timeseries_name='Q-Values',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[2]),\n",
    "    )\n",
    "\n",
    "dqs_arms = normalize(-1*np.diff(qs, axis=2))\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=dqs_arms[:, :, 0],\n",
    "    timeseries_name='dQ/dActions',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[3]),\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6868224143981934,\n",
       " 0.6868224740028381,\n",
       " 0.6868224143981934,\n",
       " 0.6868224740028381,\n",
       " 0.6868224740028381,\n",
       " 0.6868224740028381,\n",
       " 0.6868223547935486,\n",
       " 0.6868224740028381,\n",
       " 0.6868224740028381,\n",
       " 0.6868224143981934]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6664248; Time: 5.7079s; Convergence value: 3.34e-01\n",
      "Epoch 2/5 --- Loss: 0.6431982; Time: 6.3183s; Convergence value: 1.64e-01\n",
      "Epoch 3/5 --- Loss: 0.6423386; Time: 6.6044s; Convergence value: 9.70e-02\n",
      "Epoch 4/5 --- Loss: 0.6184285; Time: 6.0435s; Convergence value: 6.90e-02\n",
      "Epoch 5/5 --- Loss: 0.6121451; Time: 6.3203s; Convergence value: 1.28e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.8050s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5766s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5586s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5293s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5370s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.8307s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5963s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.6877s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5337s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5938452; Time: 0.5536s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 37.42 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "nsubmod = [4, 32, 64] # numer of submodels\n",
    "sessionlist = [32, 64] # number of sessions\n",
    "tri = [50, 200]  # number of trials\n",
    "rep = [True] # replacement\n",
    "\n",
    "for m in nsubmod:\n",
    "    for ses in sessionlist:\n",
    "        for t in tri:\n",
    "            for r in rep:\n",
    "    \n",
    "                # train model \n",
    "                train = True\n",
    "                checkpoint = False\n",
    "                data = False\n",
    "\n",
    "                path_data = 'data/dataset_train.pkl'\n",
    "                params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "                # rnn parameters\n",
    "                hidden_size = 4\n",
    "                last_output = False\n",
    "                last_state = False\n",
    "                use_lstm = False\n",
    "\n",
    "                # ensemble parameters\n",
    "                evolution_interval = None\n",
    "                sampling_replacement = bool(r)\n",
    "                n_submodels = int(m)\n",
    "                ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "                voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "                # training parameters\n",
    "                epochs = 5\n",
    "                n_steps_per_call = 16  # None for full sequence\n",
    "                batch_size = None  # None for one batch per epoch\n",
    "                learning_rate = 1e-2\n",
    "                convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "                # ground truth parameters\n",
    "                gen_alpha = .25\n",
    "                gen_beta = 3\n",
    "                forget_rate = 0.1  # possible values: 0., 0.1\n",
    "                perseverance_bias = 0.\n",
    "                correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "                # environment parameters\n",
    "                n_actions = 2\n",
    "                sigma = 0.1\n",
    "                n_trials_per_session = int(t) #200\n",
    "                n_sessions = int(ses)\n",
    "                correlated_reward = False\n",
    "                non_binary_reward = False\n",
    "\n",
    "\n",
    "                # tracked variables in the RNN\n",
    "                x_train_list = ['xQf','xQr', 'xQc']\n",
    "                control_list = ['ca','ca[k-1]', 'cr']\n",
    "                sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "                if not data:\n",
    "                    # setup\n",
    "                    environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "                    agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "                    dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "                            agent=agent,\n",
    "                            environment=environment,\n",
    "                            n_trials_per_session=n_trials_per_session,\n",
    "                            n_sessions=n_sessions,\n",
    "                            device=device)\n",
    "\n",
    "                    dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "                            agent=agent,\n",
    "                            environment=environment,\n",
    "                            n_trials_per_session=200,\n",
    "                            n_sessions=1024,\n",
    "                            device=device)\n",
    "                    \n",
    "                    params_path = rnn_utils.parameter_file_naming(\n",
    "                            'params/params',\n",
    "                            use_lstm,\n",
    "                            last_output,\n",
    "                            last_state,\n",
    "                            gen_beta,\n",
    "                            forget_rate,\n",
    "                            perseverance_bias,\n",
    "                            correlated_update,\n",
    "                            non_binary_reward,\n",
    "                            verbose=True,\n",
    "                    )\n",
    "            \n",
    "                else:\n",
    "                    # load data\n",
    "                    with open(path_data, 'rb') as f:\n",
    "                            dataset_train = pickle.load(f)\n",
    "\n",
    "                if ensemble > -1 and n_submodels == 1:\n",
    "                    Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "                    ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "                # define model\n",
    "                if use_lstm:\n",
    "                    model = rnn.LSTM(\n",
    "                            n_actions=n_actions, \n",
    "                            hidden_size=hidden_size, \n",
    "                            init_value=0.5,\n",
    "                            device=device,\n",
    "                            ).to(device)\n",
    "                else:\n",
    "                    model = [rnn.RLRNN(\n",
    "                            n_actions=n_actions, \n",
    "                            hidden_size=hidden_size, \n",
    "                            init_value=0.5,\n",
    "                            last_output=last_output,\n",
    "                            last_state=last_state,\n",
    "                            device=device,\n",
    "                            list_sindy_signals=sindy_feature_list,\n",
    "                            ).to(device)\n",
    "                                for _ in range(n_submodels)]\n",
    "\n",
    "                optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "                if checkpoint:\n",
    "                    # load trained parameters\n",
    "                    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "                    state_dict_model = state_dict['model']\n",
    "                    state_dict_optimizer = state_dict['optimizer']\n",
    "                    if isinstance(state_dict_model, dict):\n",
    "                        for m, o in zip(model, optimizer_rnn):\n",
    "                            m.load_state_dict(state_dict_model)\n",
    "                            o.load_state_dict(state_dict_optimizer)\n",
    "                    elif isinstance(state_dict_model, list):\n",
    "                        print('Loading ensemble model...')\n",
    "                        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "                            model[i].load_state_dict(state_dict_model_i)\n",
    "                            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "                        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "                    print('Loaded parameters.')\n",
    "\n",
    "\n",
    "                if train:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    #Fit the hybrid RNN\n",
    "                    print('Training the hybrid RNN...')\n",
    "                    model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "                            model=model,\n",
    "                            dataset=dataset_train,\n",
    "                            optimizer=optimizer_rnn,\n",
    "                            convergence_threshold=convergence_threshold,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            n_submodels=n_submodels,\n",
    "                            ensemble_type=ensemble,\n",
    "                            voting_type=voting_type,\n",
    "                            sampling_replacement=sampling_replacement,\n",
    "                            evolution_interval=evolution_interval,\n",
    "                            n_steps_per_call=n_steps_per_call,\n",
    "                    )\n",
    "                    \n",
    "\n",
    "                    model_name = [] # validation loss list\n",
    "\n",
    "                    # validate model\n",
    "                    print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "                    for _ in range(10):\n",
    "                        with torch.no_grad():\n",
    "                            model, _, loss = rnn_training.fit_model(\n",
    "                                    model=model,\n",
    "                                    dataset=dataset_test,\n",
    "                                    n_steps_per_call=1,\n",
    "                            )\n",
    "                        model_name.append(float(loss))\n",
    "\n",
    "\n",
    "                    print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "                    \n",
    "\n",
    "                    # save trained parameters  \n",
    "                    state_dict = {\n",
    "                        'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "                        'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "                    }\n",
    "                    torch.save(state_dict, params_path)\n",
    "                    \n",
    "                    print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "                else:\n",
    "                    model, _, _ = rnn_training.fit_model(\n",
    "                            model=model,\n",
    "                            dataset=dataset_train,\n",
    "                            epochs=0,\n",
    "                            n_submodels=n_submodels,\n",
    "                            ensemble_type=ensemble,\n",
    "                            voting_type=voting_type,\n",
    "                            verbose=True\n",
    "                    )\n",
    "\n",
    "                df = pd.DataFrame(columns=['model', 'loss','Replacement', 'Submodels', 'Voting', 'Ensemble','Sessions', 'Trials'])\n",
    "\n",
    "                df_lenght = len(model_name)\n",
    "\n",
    "                df[\"model\"] = [\"model_name\"] *df_lenght\n",
    "                df[\"loss\"] = model_name\n",
    "\n",
    "                df[\"Replacement\"] = [r] *df_lenght\n",
    "                df[\"Submodels\"] = [n_submodels] *df_lenght\n",
    "                df[\"Voting\"] = [\"median\"] *df_lenght\n",
    "                df[\"Ensemble\"] = [\"average\"] *df_lenght\n",
    "                df[\"Sessions\"] = [n_sessions] *df_lenght\n",
    "                df[\"Trials\"] = [n_trials_per_session] *df_lenght\n",
    "\n",
    "\n",
    "                df1 = pd.read_csv(\"losses.csv\")\n",
    "                losses = df1.append(df)\n",
    "\n",
    "                losses.to_csv(\"losses.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
