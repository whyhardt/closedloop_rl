{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from typing import Callable, Tuple, Iterable, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "\n",
    "# deepmind related libraries\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import pysindy as ps\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import bandits, disrnn, hybrnn, hybrnn_forget, plotting, rat_data, rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title make update rule of Q-/SINDyNetwork-Agents adjustable and make values of RNN-Agent visible\n",
    "\n",
    "class AgentQuadQ(bandits.AgentQ):\n",
    "  \n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perseveration_bias: float=0.,\n",
    "      ):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "  \n",
    "  def update(self,\n",
    "            choice: int,\n",
    "            reward: float):\n",
    "    \"\"\"Update the agent after one step of the task.\n",
    "\n",
    "    Args:\n",
    "      choice: The choice made by the agent. 0 or 1\n",
    "      reward: The reward received by the agent. 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decay q-values toward the initial value.\n",
    "    self._q = (1-self._forgetting_rate) * self._q + self._forgetting_rate * self._q_init\n",
    "\n",
    "    # Update chosen q for chosen action with observed reward.\n",
    "    self._q[choice] = self._q[choice] - self._alpha * self._q[choice]**2 + self._alpha * reward\n",
    "\n",
    "\n",
    "class AgentSindy(bandits.AgentQ):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      alpha: float=0.2,\n",
    "      beta: float=3.,\n",
    "      n_actions: int=2,\n",
    "      forgetting_rate: float=0.,\n",
    "      perservation_bias: float=0.,):\n",
    "    super().__init__(alpha, beta, n_actions, forgetting_rate, perservation_bias)\n",
    "\n",
    "    self._update_rule = lambda q, choice, reward: (1 - self._alpha) * q[choice] + self._alpha * reward\n",
    "    self._update_rule_formula = None\n",
    "\n",
    "  def set_update_rule(self, update_rule: callable, update_rule_formula: str=None):\n",
    "    self._update_rule=update_rule\n",
    "    self._update_rule_formula=update_rule_formula\n",
    "\n",
    "  @property\n",
    "  def update_rule(self):\n",
    "    if self._update_rule_formula is not None:\n",
    "      return self._update_rule_formula\n",
    "    else:\n",
    "      return f'{self._update_rule}'\n",
    "\n",
    "  def update(self, choice: int, reward: int):\n",
    "\n",
    "    for c in range(self._n_actions):\n",
    "      self._q[c] = self._update_rule(self._q[c], int(c==choice), reward)\n",
    "\n",
    "\n",
    "class AgentNetwork_VisibleState(bandits.AgentNetwork):\n",
    "\n",
    "  def __init__(self,\n",
    "               make_network: Callable[[], hk.RNNCore],\n",
    "               params: hk.Params,\n",
    "               n_actions: int = 2,\n",
    "               state_to_numpy: bool = False,\n",
    "               habit=False):\n",
    "    super().__init__(make_network=make_network, params=params, n_actions=n_actions, state_to_numpy=state_to_numpy)\n",
    "    self.habit = habit\n",
    "\n",
    "  @property\n",
    "  def q(self):\n",
    "    if self.habit:\n",
    "      return self._state[2], self._state[3]\n",
    "    else:\n",
    "      return self._state[3].reshape(-1)\n",
    "\n",
    "dict_agents = {\n",
    "    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n",
    "    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "\n",
    "dataset_type = 'synt'  #@param ['synt', 'real']\n",
    "\n",
    "#@markdown Set up parameters for synthetic data generation:\n",
    "if dataset_type == 'synt':\n",
    "    # agent parameters\n",
    "    agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n",
    "    gen_alpha = .25 #@param\n",
    "    gen_beta = 5 #@param\n",
    "    forgetting_rate = 0.1 #@param\n",
    "    perseveration_bias = 0.  #@param\n",
    "    # environment parameters\n",
    "    non_binary_reward = False #@param\n",
    "    n_actions = 2 #@param\n",
    "    sigma = .1  #@param\n",
    "    \n",
    "    # experiement parameters\n",
    "    n_trials_per_session = 200  #@param\n",
    "    n_sessions = 220  #@param\n",
    "    \n",
    "    # setup\n",
    "    environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n",
    "    agent = dict_agents[agent_kw](gen_alpha, gen_beta, n_actions, forgetting_rate, perseveration_bias)  \n",
    "  \n",
    "    dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "        agent=agent,\n",
    "        environment=environment,\n",
    "        n_trials_per_session=n_trials_per_session,\n",
    "        n_sessions=n_sessions)\n",
    "\n",
    "    dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "        agent=agent,\n",
    "        environment=environment,\n",
    "        n_trials_per_session=n_trials_per_session,\n",
    "        n_sessions=n_sessions)\n",
    "\n",
    "#@markdown Set up parameters for loading rat data from Miller et al 2019.\n",
    "elif dataset_type == 'real':\n",
    "    # TODO: ys are not the rewards but the following choices!!!!\n",
    "    raise NotImplementedError('This is not implemented yet.')\n",
    "\n",
    "else:\n",
    "  raise NotImplementedError(\n",
    "      (f'dataset_type {dataset_type} not implemented. '\n",
    "       'Please select from drop-down list.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up Hybrid RNN.\n",
    "\n",
    "#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n",
    "use_hidden_state = False  #@param ['True', 'False']\n",
    "\n",
    "#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n",
    "use_previous_values = False  #@param ['True', 'False']\n",
    "\n",
    "#@markdown If True, learn a value for the forgetting term\n",
    "fit_forget = False  #@param ['True', 'False']\n",
    "\n",
    "#@markdown Learn a reward-independent term that depends on past choices.\n",
    "habit_weight = \"0\"  #@param [0, 1]\n",
    "habit_weight = float(habit_weight)\n",
    "\n",
    "value_weight = 1.  # This is needed for it to be doing RL\n",
    "\n",
    "rnn_rl_params = {\n",
    "    's': use_hidden_state,\n",
    "    'o': use_previous_values,\n",
    "    'fit_forget': fit_forget,\n",
    "    'forget': 0.,\n",
    "    'w_h': habit_weight,\n",
    "    'w_v': value_weight}\n",
    "network_params = {'n_actions': n_actions, 'hidden_size': 16}\n",
    "\n",
    "def make_hybrnn():\n",
    "  # model = hybrnn.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n",
    "  model = hybrnn_forget.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n",
    "  return model\n",
    "\n",
    "optimizer_rnn = optax.adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the hybrid RNN...\n",
      "Step 500 of 500; Loss: 0.5324958; Time: 10.9s)\n",
      "Model not yet converged - Running more steps of gradient descent. Time elapsed = 2e-05s.\n",
      "Step 500 of 500; Loss: 0.5321959; Time: 10.5s)\n",
      "Model not yet converged (convergence_value = 0.0005632543) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5321779; Time: 10.9s)\n",
      "Model not yet converged (convergence_value = 3.382327e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5321552; Time: 10.6s)\n",
      "Model not yet converged (convergence_value = 4.267252e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5321294; Time: 10.7s)\n",
      "Model not yet converged (convergence_value = 4.838665e-05) - Running more steps of gradient descent. Time elapsed = 0.0002s.\n",
      "Step 500 of 500; Loss: 0.5321042; Time: 10.6s)\n",
      "Model not yet converged (convergence_value = 4.738089e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5320842; Time: 10.4s)\n",
      "Model not yet converged (convergence_value = 3.763767e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5320711; Time: 10.9s)\n",
      "Model not yet converged (convergence_value = 2.453262e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5320619; Time: 10.6s)\n",
      "Model not yet converged (convergence_value = 1.736369e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5320503; Time: 10.8s)\n",
      "Model not yet converged (convergence_value = 2.184503e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5320330; Time: 10.3s)\n",
      "Model not yet converged (convergence_value = 3.248818e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5320100; Time: 10.8s)\n",
      "Model not yet converged (convergence_value = 4.313227e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5319883; Time: 10.5s)\n",
      "Model not yet converged (convergence_value = 4.078136e-05) - Running more steps of gradient descent. Time elapsed = 0.0002s.\n",
      "Step 500 of 500; Loss: 0.5319719; Time: 11.6s)\n",
      "Model not yet converged (convergence_value = 3.081135e-05) - Running more steps of gradient descent. Time elapsed = 5e-05s.\n",
      "Step 500 of 500; Loss: 0.5319584; Time: 10.9s)\n",
      "Model not yet converged (convergence_value = 2.543415e-05) - Running more steps of gradient descent. Time elapsed = 4e-05s.\n",
      "Step 500 of 500; Loss: 0.5319496; Time: 11.6s)\n",
      "Model not yet converged (convergence_value = 1.647099e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5319428; Time: 11.4s)\n",
      "Model not yet converged (convergence_value = 1.288568e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5319373; Time: 12.6s)\n",
      "Model not yet converged (convergence_value = 1.030868e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n",
      "Step 500 of 500; Loss: 0.5319339; Time: 11.6s)\n",
      "Model Converged! Time elapsed = 2e-05s.\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "load = False  # only relevant if train is True --> Determines whether to load trained parameters and continue training or start new training\n",
    "\n",
    "# params_path = 'params/params_rnn_forget_f01.pkl'\n",
    "params_path = 'params/params_rnn_forget_f01_b5.pkl'\n",
    "\n",
    "if train:\n",
    "  if load:\n",
    "    with open(params_path, 'rb') as f:\n",
    "      rnn_params = pickle.load(f)\n",
    "    opt_state = rnn_params[1]\n",
    "    rnn_params = rnn_params[0]\n",
    "    print('Loaded parameters.')\n",
    "  else:\n",
    "    opt_state = None\n",
    "    rnn_params = None\n",
    "\n",
    "  # with jax.disable_jit():\n",
    "  #@title Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  rnn_params, opt_state, _ = rnn_utils.fit_model(\n",
    "      model_fun=make_hybrnn,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      optimizer_state=opt_state,\n",
    "      model_params=rnn_params,\n",
    "      loss_fun='categorical',  # penalized_categorical, categorical\n",
    "      convergence_thresh=1e-5,\n",
    "      n_steps_max=10000,\n",
    "  )\n",
    "\n",
    "  # save trained parameters\n",
    "  params = (rnn_params, opt_state)\n",
    "  with open(params_path, 'wb') as f:\n",
    "    pickle.dump(params, f)\n",
    "    \n",
    "else:\n",
    "  # load trained parameters\n",
    "  with open(params_path, 'rb') as f:\n",
    "    rnn_params = pickle.load(f)[0]\n",
    "  print('Loaded parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINDY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sindy_data(\n",
    "    dataset,\n",
    "    agent: bandits.AgentQ,\n",
    "    sessions=-1,\n",
    "    get_choices=True,\n",
    "    # keep_sessions=False,\n",
    "    ):\n",
    "\n",
    "  # Get training data for SINDy\n",
    "  # put all relevant signals in x_train\n",
    "\n",
    "  if not isinstance(sessions, Iterable) and sessions == -1:\n",
    "    # use all sessions\n",
    "    sessions = np.arange(len(dataset))\n",
    "  else:\n",
    "    # use only the specified sessions\n",
    "    sessions = np.array(sessions)\n",
    "    \n",
    "  if get_choices:\n",
    "    n_control = 2\n",
    "  else:\n",
    "    n_control = 1\n",
    "  \n",
    "  # if keep_sessions:\n",
    "  #   # concatenate all sessions along the trial dimensinon -> shape: (n_trials, n_sessions, n_features)\n",
    "  #   choices = np.expand_dims(np.stack([dataset[i].choices for i in sessions], axis=1), -1)\n",
    "  #   rewards = np.expand_dims(np.stack([dataset[i].rewards for i in sessions], axis=1), -1)\n",
    "  #   qs = np.stack([dataset[i].q for i in sessions], axis=1)\n",
    "  # else:\n",
    "  # concatenate all sessions along the trial dimensinon -> shape: (n_trials*n_sessions, n_features)\n",
    "  # choices = np.expand_dims(np.concatenate([dataset[i].choices for i in sessions], axis=0), -1)\n",
    "  # rewards = np.expand_dims(np.concatenate([dataset[i].rewards for i in sessions], axis=0), -1)\n",
    "  # qs = np.concatenate([dataset[i].q for i in sessions], axis=0)\n",
    "  \n",
    "  choices = np.stack([dataset[i].choices for i in sessions], axis=0)\n",
    "  rewards = np.stack([dataset[i].rewards for i in sessions], axis=0)\n",
    "  qs = np.stack([dataset[i].q for i in sessions], axis=0)\n",
    "  \n",
    "  if not get_choices:\n",
    "    raise NotImplementedError('Only get_choices=True is implemented right now.')\n",
    "    n_sessions = qs.shape[0]\n",
    "    n_trials = qs.shape[1]*qs.shape[2]\n",
    "    qs_all = np.zeros((n_sessions, n_trials))\n",
    "    r_all = np.zeros((n_sessions, n_trials))\n",
    "    c_all = None\n",
    "    # concatenate the data of all arms into one array for more training data\n",
    "    index_end_last_arm = 0\n",
    "    for index_arm in range(agent._n_actions):\n",
    "      index = np.where(choices==index_arm)[0]\n",
    "      r_all[index_end_last_arm:index_end_last_arm+len(index)] = rewards[index]\n",
    "      qs_all[index_end_last_arm:index_end_last_arm+len(index)] = qs[index, index_arm].reshape(-1, 1)\n",
    "      index_end_last_arm += len(index)\n",
    "  else:\n",
    "    choices_oh = np.zeros((len(sessions), choices.shape[1], agent._n_actions))\n",
    "    for sess in sessions:\n",
    "      # one-hot encode choices\n",
    "      choices_oh[sess] = np.eye(agent._n_actions)[choices[sess]]\n",
    "      # add choices as control parameter; no sorting required then\n",
    "      # qs_all = np.concatenate([qs[sess, :, i] for i in range(agent._n_actions)], axis=1)\n",
    "      # c_all = np.concatenate([choices[:, sess, i] for i in range(agent._n_actions)], axis=1)\n",
    "      # r_all = np.concatenate([rewards for _ in range(agent._n_actions)], axis=1)\n",
    "      # concatenate all qs values of one sessions along the trial dimension\n",
    "      qs_all = np.concatenate([np.stack([np.expand_dims(qs_sess[:, i], axis=-1) for i in range(agent._n_actions)], axis=0) for qs_sess in qs], axis=0)\n",
    "      c_all = np.concatenate([np.stack([c_sess[:, i] for i in range(agent._n_actions)], axis=0) for c_sess in choices_oh], axis=0)\n",
    "      r_all = np.concatenate([np.stack([r_sess for _ in range(agent._n_actions)], axis=0) for r_sess in rewards], axis=0)\n",
    "  \n",
    "  # get observed dynamics\n",
    "  x_train = qs_all\n",
    "  feature_names = ['q']\n",
    "\n",
    "  # get control\n",
    "  control_names = []\n",
    "  control = np.zeros((*x_train.shape[:-1], n_control))\n",
    "  if get_choices:\n",
    "    control[:, :, 0] = c_all\n",
    "    control_names += ['c']\n",
    "  control[:, :, n_control-1] = r_all\n",
    "  control_names += ['r']\n",
    "  \n",
    "  feature_names += control_names\n",
    "  \n",
    "  print(f'Shape of Q-Values is: {x_train.shape}')\n",
    "  print(f'Shape of control parameters is: {control.shape}')\n",
    "  print(f'Feature names are: {feature_names}')\n",
    "  \n",
    "  # make x_train and control sequences instead of arrays\n",
    "  x_train = [x_train_sess for x_train_sess in x_train]\n",
    "  control = [control_sess for control_sess in control]\n",
    " \n",
    "  return x_train, control, feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiNDY Ground truth dataset fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q-Values is: (440, 200, 1)\n",
      "Shape of control parameters is: (440, 200, 2)\n",
      "Feature names are: ['q', 'c', 'r']\n",
      " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
      "         0 ... 7.0994e+00 ... 8.3619e-02 ...          9 ... 7.1830e+00\n",
      "         1 ... 1.0142e+00 ... 8.7461e-02 ...          8 ... 1.1016e+00\n",
      "         2 ... 3.6680e-01 ... 8.6815e-02 ...          8 ... 4.5361e-01\n",
      "(q)[k+1] = 0.047 1 + 0.903 q[k] + 152632824.330 q[k] c[k] + -2321015003.621 c[k] r[k] + 0.010 q[k]^3 + -152632824.577 q[k] c[k]^2 + 4492037219.974 c[k]^2 r[k] + -2171022216.103 c[k] r[k]^2\n"
     ]
    }
   ],
   "source": [
    "#@title Fit SINDy to actual dataset\n",
    "# library = custom_lib  # custom_lib, poly_lib, solution_lib\n",
    "ensemble = False\n",
    "library_ensemble = False\n",
    "\n",
    "get_choices = True\n",
    "poly_order = 3\n",
    "threshold = 0.01\n",
    "dt = 1\n",
    "\n",
    "# library_datasindy = ps.CustomLibrary(\n",
    "#     library_functions=custom_lib_functions,\n",
    "#     function_names=custom_lib_names,\n",
    "#     include_bias=True,\n",
    "# )\n",
    "\n",
    "library_datasindy = ps.PolynomialLibrary(poly_order)\n",
    "\n",
    "experiment_list_datasindy = None\n",
    "\n",
    "if dataset_type == 'synt':\n",
    "    x_train, control, feature_names = make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n",
    "\n",
    "    datasindy = ps.SINDy(\n",
    "        optimizer=ps.STLSQ(threshold=threshold, verbose=True, alpha=0.1),\n",
    "        feature_library=library_datasindy,\n",
    "        discrete_time=True,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    "    datasindy.fit(x_train, t=dt, u=control, ensemble=ensemble, library_ensemble=library_ensemble, multiple_trajectories=True)\n",
    "    datasindy.print()\n",
    "\n",
    "    # set new sindy update rule and synthesize new dataset\n",
    "    if not get_choices:\n",
    "        update_rule_datasindy = lambda q, choice, reward: datasindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n",
    "    else:\n",
    "        update_rule_datasindy = lambda q, choice, reward: datasindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n",
    "    \n",
    "    datasindyagent = AgentSindy(alpha=0, beta=gen_beta, n_actions=n_actions)\n",
    "    datasindyagent.set_update_rule(update_rule_datasindy)\n",
    "\n",
    "    # _, experiment_list_datasindy = bandits.create_dataset(datasindyagent, environment, n_trials_per_session, n_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiNDY fitt to RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is RNN to SiNDY\n",
    "#@title Synthesize a dataset using the fitted network\n",
    "hybrnn_agent = AgentNetwork_VisibleState(make_hybrnn, rnn_params, habit=habit_weight==1, n_actions=n_actions)\n",
    "dataset_hybrnn, experiment_list_hybrnn = bandits.create_dataset(hybrnn_agent, environment, n_trials_per_session, int(n_sessions*1e0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q-Values is: (440, 200, 1)\n",
      "Shape of control parameters is: (440, 200, 2)\n",
      "Feature names are: ['q', 'c', 'r']\n",
      "Dataset characteristics: max=3.0769786834716797, min=-2.972050905227661\n",
      "(q)[k+1] = 0.362 1 + 0.214 q[k] + -19048347826.358 c[k] + 0.060 q[k]^2 + -187415.909 q[k] c[k] + 38055833457.122 c[k]^2 + 25102425487.198 c[k] r[k] + -0.186 q[k]^2 c[k] + 187416.532 q[k] c[k]^2 + -19007485631.102 c[k]^3 + -12625863212.696 c[k]^2 r[k] + -12476562274.304 c[k] r[k]^2\n",
      "Sparsity index: 0.7\n"
     ]
    }
   ],
   "source": [
    "#@title Fit SINDy to RNN data and synthesize new dataset\n",
    "\n",
    "threshold = 0.015\n",
    "\n",
    "x_train, control, feature_names = make_sindy_data(experiment_list_hybrnn, hybrnn_agent, get_choices=get_choices)\n",
    "# x_train, control, feature_names = make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n",
    "# scale q-values between 0 and 1 for more realistic dynamics\n",
    "x_max = np.max(np.stack(x_train, axis=0))\n",
    "x_min = np.min(np.stack(x_train, axis=0))\n",
    "print(f'Dataset characteristics: max={x_max}, min={x_min}')\n",
    "x_train = [(x - x_min) / (x_max - x_min) for x in x_train]\n",
    "\n",
    "# library_rnnsindy = ps.CustomLibrary(\n",
    "#     library_functions=custom_lib_functions,\n",
    "#     function_names=custom_lib_names,\n",
    "#     include_bias=True,\n",
    "# )\n",
    "\n",
    "library_rnnsindy = ps.PolynomialLibrary(poly_order)\n",
    "\n",
    "rnnsindy = ps.SINDy(\n",
    "    optimizer=ps.STLSQ(threshold=threshold, verbose=False, alpha=0.1),\n",
    "    feature_library=library_rnnsindy,\n",
    "    discrete_time=True,\n",
    "    feature_names=feature_names,\n",
    ")\n",
    "\n",
    "rnnsindy.fit(x_train, t=dt, u=control, ensemble=True, library_ensemble=False, multiple_trajectories=True)\n",
    "rnnsindy.print()\n",
    "sparsity_index = np.sum(rnnsindy.coefficients() < threshold) / rnnsindy.coefficients().size\n",
    "print(f'Sparsity index: {sparsity_index}')\n",
    "\n",
    "if not get_choices:\n",
    "    update_rule_rnnsindy = lambda q, choice, reward: rnnsindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n",
    "else:\n",
    "    update_rule_rnnsindy = lambda q, choice, reward: rnnsindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n",
    "\n",
    "rnnsindyagent = AgentSindy(alpha=0, beta=1, n_actions=n_actions)\n",
    "rnnsindyagent.set_update_rule(update_rule_rnnsindy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labrotation-daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
