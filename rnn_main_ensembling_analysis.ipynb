{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RL libraries\n",
    "sys.path.append('resources')  # add source directoy to path\n",
    "from resources import rnn, rnn_training, bandits, rnn_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6085727; Time: 18.1230s; Convergence value: 3.91e-01\n",
      "Epoch 2/5 --- Loss: 0.6095850; Time: 14.9941s; Convergence value: 1.78e-01\n",
      "Epoch 3/5 --- Loss: 0.6080438; Time: 13.7053s; Convergence value: 1.05e-01\n",
      "Epoch 4/5 --- Loss: 0.6078517; Time: 11.4109s; Convergence value: 6.60e-02\n",
      "Epoch 5/5 --- Loss: 0.6076877; Time: 13.4029s; Convergence value: 6.19e-04\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 1.3506s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.7666s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.7329s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.7352s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.9492s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.7791s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.6491s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.6835s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.7014s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5965887; Time: 0.6799s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 79.71 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4e6a552e49e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;31m# get q-values from trained rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m \u001b[0mqs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_update_dynamics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_list_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msession_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[0mlist_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[0mlist_qs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\bandits.py\u001b[0m in \u001b[0;36mget_update_dynamics\u001b[1;34m(experiment, agent)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[0mqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[0mchoice_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_choice_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\bandits.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, choice, reward)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[1;31m# self.set_state(self._model.get_state(return_dict=True))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, prev_state, batch_first)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 \u001b[0maction_oh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_oh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "\"\"\" # train model (BASELINE)\n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 1\n",
    "ensemble = rnn_training.ensemble_types.NONE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  baseline_losses = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      baseline_losses.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2)\n",
    "\n",
    "# Analysis\n",
    "session_id = 0\n",
    "\n",
    "choices = experiment_list_test[session_id].choices\n",
    "rewards = experiment_list_test[session_id].rewards\n",
    "\n",
    "list_probs = []\n",
    "list_qs = []\n",
    "\n",
    "# get q-values from groundtruth\n",
    "qs_test, probs_test = bandits.get_update_dynamics(experiment_list_test[session_id], agent)\n",
    "list_probs.append(np.expand_dims(probs_test, 0))\n",
    "list_qs.append(np.expand_dims(qs_test, 0))\n",
    "\n",
    "# get q-values from trained rnn\n",
    "qs_rnn, probs_rnn = bandits.get_update_dynamics(experiment_list_test[session_id], rnn_agent)\n",
    "list_probs.append(np.expand_dims(probs_rnn, 0))\n",
    "list_qs.append(np.expand_dims(qs_rnn, 0))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:grey']\n",
    "\n",
    "# concatenate all choice probs and q-values\n",
    "probs = np.concatenate(list_probs, axis=0)\n",
    "qs = np.concatenate(list_qs, axis=0)\n",
    "\n",
    "# normalize q-values\n",
    "def normalize(qs):\n",
    "  return (qs - np.min(qs, axis=1, keepdims=True)) / (np.max(qs, axis=1, keepdims=True) - np.min(qs, axis=1, keepdims=True))\n",
    "\n",
    "qs = normalize(qs)\n",
    "fig, axs = plt.subplots(4, 1, figsize=(20, 10))\n",
    "\n",
    "reward_probs = np.stack([experiment_list_test[session_id].timeseries[:, i] for i in range(n_actions)], axis=0)\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=reward_probs,\n",
    "    timeseries_name='Reward Probs',\n",
    "    labels=[f'Arm {a}' for a in range(n_actions)],\n",
    "    color=['tab:purple', 'tab:cyan'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[0]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=probs[:, :, 0],\n",
    "    timeseries_name='Choice Probs',\n",
    "    color=colors,\n",
    "    labels=['Ground Truth', 'RNN'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[1]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=qs[:, :, 0],\n",
    "    timeseries_name='Q-Values',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[2]),\n",
    "    )\n",
    "\n",
    "dqs_arms = normalize(-1*np.diff(qs, axis=2))\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=dqs_arms[:, :, 0],\n",
    "    timeseries_name='dQ/dActions',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[3]),\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" baseline_losses\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"Baseline\"]*len(baseline_losses)\n",
    "df[\"loss\"] = baseline_losses\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2)\n",
    "\n",
    "# Analysis\n",
    "session_id = 0\n",
    "\n",
    "choices = experiment_list_test[session_id].choices\n",
    "rewards = experiment_list_test[session_id].rewards\n",
    "\n",
    "list_probs = []\n",
    "list_qs = []\n",
    "\n",
    "# get q-values from groundtruth\n",
    "qs_test, probs_test = bandits.get_update_dynamics(experiment_list_test[session_id], agent)\n",
    "list_probs.append(np.expand_dims(probs_test, 0))\n",
    "list_qs.append(np.expand_dims(qs_test, 0))\n",
    "\n",
    "# get q-values from trained rnn\n",
    "qs_rnn, probs_rnn = bandits.get_update_dynamics(experiment_list_test[session_id], rnn_agent)\n",
    "list_probs.append(np.expand_dims(probs_rnn, 0))\n",
    "list_qs.append(np.expand_dims(qs_rnn, 0))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:grey']\n",
    "\n",
    "# concatenate all choice probs and q-values\n",
    "probs = np.concatenate(list_probs, axis=0)\n",
    "qs = np.concatenate(list_qs, axis=0)\n",
    "\n",
    "# normalize q-values\n",
    "def normalize(qs):\n",
    "  return (qs - np.min(qs, axis=1, keepdims=True)) / (np.max(qs, axis=1, keepdims=True) - np.min(qs, axis=1, keepdims=True))\n",
    "\n",
    "qs = normalize(qs)\n",
    "fig, axs = plt.subplots(4, 1, figsize=(20, 10))\n",
    "\n",
    "reward_probs = np.stack([experiment_list_test[session_id].timeseries[:, i] for i in range(n_actions)], axis=0)\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=reward_probs,\n",
    "    timeseries_name='Reward Probs',\n",
    "    labels=[f'Arm {a}' for a in range(n_actions)],\n",
    "    color=['tab:purple', 'tab:cyan'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[0]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=probs[:, :, 0],\n",
    "    timeseries_name='Choice Probs',\n",
    "    color=colors,\n",
    "    labels=['Ground Truth', 'RNN'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[1]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=qs[:, :, 0],\n",
    "    timeseries_name='Q-Values',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[2]),\n",
    "    )\n",
    "\n",
    "dqs_arms = normalize(-1*np.diff(qs, axis=2))\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=dqs_arms[:, :, 0],\n",
    "    timeseries_name='dQ/dActions',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[3]),\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6424130; Time: 308.1779s; Convergence value: 3.58e-01\n",
      "Epoch 2/5 --- Loss: 0.5910122; Time: 362.1660s; Convergence value: 1.91e-01\n",
      "Epoch 3/5 --- Loss: 0.5868228; Time: 362.2209s; Convergence value: 1.14e-01\n",
      "Epoch 4/5 --- Loss: 0.5854608; Time: 348.2068s; Convergence value: 7.26e-02\n",
      "Epoch 5/5 --- Loss: 0.5849690; Time: 321.2144s; Convergence value: 1.00e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 2.0071s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.7294s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6976s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152542; Time: 0.7176s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6984s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6662s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6516s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6708s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6267s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6152543; Time: 0.6397s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 1754.60 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" # 32 submodels, ensemble average\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 32\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n32_eA_loss = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n32_eA_loss.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-40b5db348e3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# get q-values from trained rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mqs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_update_dynamics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_list_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msession_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mlist_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mlist_qs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\bandits.py\u001b[0m in \u001b[0;36mget_update_dynamics\u001b[1;34m(experiment, agent)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[0mqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[0mchoice_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_choice_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\bandits.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, choice, reward)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[1;31m# self.set_state(self._model.get_state(return_dict=True))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\Desktop\\Uni\\Sync\\8_Semester\\Lab rotation modeling\\closedloop_rl\\resources\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, prev_state, batch_first)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 \u001b[0maction_oh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_oh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "# Synthesize a dataset using the fitted network\n",
    "environment = bandits.EnvironmentBanditsDrift(0.1)\n",
    "model.set_device(torch.device('cpu'))\n",
    "model.to(torch.device('cpu'))\n",
    "rnn_agent = bandits.AgentNetwork(model, n_actions=2)\n",
    "\n",
    "# Analysis\n",
    "session_id = 0\n",
    "\n",
    "choices = experiment_list_test[session_id].choices\n",
    "rewards = experiment_list_test[session_id].rewards\n",
    "\n",
    "list_probs = []\n",
    "list_qs = []\n",
    "\n",
    "# get q-values from groundtruth\n",
    "qs_test, probs_test = bandits.get_update_dynamics(experiment_list_test[session_id], agent)\n",
    "list_probs.append(np.expand_dims(probs_test, 0))\n",
    "list_qs.append(np.expand_dims(qs_test, 0))\n",
    "\n",
    "# get q-values from trained rnn\n",
    "qs_rnn, probs_rnn = bandits.get_update_dynamics(experiment_list_test[session_id], rnn_agent)\n",
    "list_probs.append(np.expand_dims(probs_rnn, 0))\n",
    "list_qs.append(np.expand_dims(qs_rnn, 0))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:grey']\n",
    "\n",
    "# concatenate all choice probs and q-values\n",
    "probs = np.concatenate(list_probs, axis=0)\n",
    "qs = np.concatenate(list_qs, axis=0)\n",
    "\n",
    "# normalize q-values\n",
    "def normalize(qs):\n",
    "  return (qs - np.min(qs, axis=1, keepdims=True)) / (np.max(qs, axis=1, keepdims=True) - np.min(qs, axis=1, keepdims=True))\n",
    "\n",
    "qs = normalize(qs)\n",
    "fig, axs = plt.subplots(4, 1, figsize=(20, 10))\n",
    "\n",
    "reward_probs = np.stack([experiment_list_test[session_id].timeseries[:, i] for i in range(n_actions)], axis=0)\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=reward_probs,\n",
    "    timeseries_name='Reward Probs',\n",
    "    labels=[f'Arm {a}' for a in range(n_actions)],\n",
    "    color=['tab:purple', 'tab:cyan'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[0]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=probs[:, :, 0],\n",
    "    timeseries_name='Choice Probs',\n",
    "    color=colors,\n",
    "    labels=['Ground Truth', 'RNN'],\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[1]),\n",
    "    )\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=qs[:, :, 0],\n",
    "    timeseries_name='Q-Values',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[2]),\n",
    "    )\n",
    "\n",
    "dqs_arms = normalize(-1*np.diff(qs, axis=2))\n",
    "\n",
    "bandits.plot_session(\n",
    "    compare=True,\n",
    "    choices=choices,\n",
    "    rewards=rewards,\n",
    "    timeseries=dqs_arms[:, :, 0],\n",
    "    timeseries_name='dQ/dActions',\n",
    "    color=colors,\n",
    "    binary=not non_binary_reward,\n",
    "    fig_ax=(fig, axs[3]),\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n32_eA_rF_vMedian\"]*len(n32_eA_loss)\n",
    "df[\"loss\"] = n32_eA_loss\n",
    "\n",
    "df.to_csv(\"losses.csv\", index=False) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6656887; Time: 149.2490s; Convergence value: 3.34e-01\n",
      "Epoch 2/5 --- Loss: 0.5774788; Time: 170.3928s; Convergence value: 2.00e-01\n",
      "Epoch 3/5 --- Loss: 0.5747513; Time: 179.9744s; Convergence value: 1.20e-01\n",
      "Epoch 4/5 --- Loss: 0.5744476; Time: 167.1248s; Convergence value: 7.62e-02\n",
      "Epoch 5/5 --- Loss: 0.5742643; Time: 156.3004s; Convergence value: 1.55e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 1.6211s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.4725s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.5172s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.5093s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.7930s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.7204s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.9073s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.7242s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972804; Time: 0.7155s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5972803; Time: 0.8151s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 838.32 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" # 16 submodels, ensemble average\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 16\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n16_eA_loss = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n16_eA_loss.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n16_eA_loss\"]*len(n16_eA_loss)\n",
    "df[\"loss\"] = n16_eA_loss\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.5625863; Time: 40.1743s; Convergence value: 4.37e-01\n",
      "Epoch 2/5 --- Loss: 0.5621565; Time: 43.2809s; Convergence value: 1.99e-01\n",
      "Epoch 3/5 --- Loss: 0.5623043; Time: 43.7317s; Convergence value: 1.17e-01\n",
      "Epoch 4/5 --- Loss: 0.5623129; Time: 38.9261s; Convergence value: 7.30e-02\n",
      "Epoch 5/5 --- Loss: 0.5622482; Time: 42.4550s; Convergence value: 1.28e-04\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5826510; Time: 1.1511s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.8015s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.4829s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.4844s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.5312s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826510; Time: 0.5440s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.5144s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826510; Time: 0.5208s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.5157s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5826511; Time: 0.5887s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 215.72 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" # 4 submodels, ensemble average\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_loss = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_loss.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMedian_loss\"]*len(n4_eA_loss)\n",
    "df[\"loss\"] = n4_eA_loss\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6473184; Time: 38.7940s; Convergence value: 3.53e-01\n",
      "Epoch 2/5 --- Loss: 0.5104391; Time: 49.3358s; Convergence value: 2.35e-01\n",
      "Epoch 3/5 --- Loss: 0.5069841; Time: 44.6704s; Convergence value: 1.41e-01\n",
      "Epoch 4/5 --- Loss: 0.5057298; Time: 41.4925s; Convergence value: 9.06e-02\n",
      "Epoch 5/5 --- Loss: 0.5052085; Time: 41.6943s; Convergence value: 2.41e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 1.5447s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 0.6642s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 0.6583s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 0.5843s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 1.0439s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006865; Time: 0.7657s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006865; Time: 0.5783s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 0.7070s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 0.7821s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6006864; Time: 1.1556s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 225.51 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, MEAN\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_mean_loss = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_mean_loss.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_mean_loss\"]*len(n4_eA_mean_loss)\n",
    "df[\"loss\"] = n4_eA_mean_loss\n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacement = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.9813210; Time: 44.3188s; Convergence value: 1.87e-02\n",
      "Epoch 2/5 --- Loss: 0.5939740; Time: 36.5132s; Convergence value: 2.20e-01\n",
      "Epoch 3/5 --- Loss: 0.6020821; Time: 35.0467s; Convergence value: 1.37e-01\n",
      "Epoch 4/5 --- Loss: 0.6115658; Time: 44.1253s; Convergence value: 9.46e-02\n",
      "Epoch 5/5 --- Loss: 0.6077671; Time: 35.1974s; Convergence value: 7.03e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 1.0227s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5679s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5664s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5662s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5665s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.6237s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5756s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5637s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.7334s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5912160; Time: 0.5686s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 202.42 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" # 4 submodels, ensemble average, replacement true\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = True\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rT_vA_loss = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rT_vA_loss.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n4_eA_rT_vA_loss\n",
    "\n",
    "\"\"\" df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rT_vA_loss\"]*len(n4_eA_rT_vA_loss)\n",
    "df[\"loss\"] = n4_eA_rT_vA_loss\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# until HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7806702; Time: 156.4051s; Convergence value: 2.19e-01\n",
      "Epoch 2/5 --- Loss: 0.6086048; Time: 195.4610s; Convergence value: 1.94e-01\n",
      "Epoch 3/5 --- Loss: 0.6179161; Time: 174.1217s; Convergence value: 1.20e-01\n",
      "Epoch 4/5 --- Loss: 0.6172372; Time: 174.7405s; Convergence value: 7.76e-02\n",
      "Epoch 5/5 --- Loss: 0.6013812; Time: 169.7018s; Convergence value: 3.62e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 1.4950s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.9518s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.7560s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.8240s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.6556s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.7185s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.6249s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.8037s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 1.0287s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916854; Time: 0.6777s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 887.28 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'losses.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4c3049555ffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"losses.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\meril\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             )\n\u001b[0;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\meril\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'losses.csv'"
     ]
    }
   ],
   "source": [
    "# 16 submodels, ensemble average, replacement true\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = True\n",
    "n_submodels = 16\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n16_eA_rT_vA = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n16_eA_rT_vA.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n16_eA_rT_vA\"]*len(n16_eA_rT_vA)\n",
    "df[\"loss\"] = n16_eA_rT_vA\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7097447; Time: 388.3936s; Convergence value: 2.90e-01\n",
      "Epoch 2/5 --- Loss: 0.5554522; Time: 376.2014s; Convergence value: 2.16e-01\n",
      "Epoch 3/5 --- Loss: 0.5579701; Time: 326.1727s; Convergence value: 1.30e-01\n",
      "Epoch 4/5 --- Loss: 0.5586378; Time: 348.1144s; Convergence value: 8.36e-02\n",
      "Epoch 5/5 --- Loss: 0.5581956; Time: 332.8312s; Convergence value: 2.66e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 1.2164s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.6356s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.5270s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050162; Time: 0.8517s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.5345s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.5191s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.4849s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.5497s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050162; Time: 0.5339s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050161; Time: 0.5658s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 1793.43 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 32 submodels, ensemble average, replacement true, Mmedian\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = True\n",
    "n_submodels = 32\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n32_eA_rT_vMed = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n32_eA_rT_vMed.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n32_eA_rT_vMed\"]*len(n32_eA_rT_vMed)\n",
    "df[\"loss\"] = n32_eA_rT_vMed\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7676185; Time: 29.5787s; Convergence value: 2.32e-01\n",
      "Epoch 2/5 --- Loss: 0.6138930; Time: 29.1224s; Convergence value: 1.89e-01\n",
      "Epoch 3/5 --- Loss: 0.6126850; Time: 28.9014s; Convergence value: 1.14e-01\n",
      "Epoch 4/5 --- Loss: 0.6008949; Time: 28.5122s; Convergence value: 7.72e-02\n",
      "Epoch 5/5 --- Loss: 0.5930960; Time: 28.3805s; Convergence value: 3.18e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.7832s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5164s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5945s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5198s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5297s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.4998s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029520; Time: 0.6665s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5694s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5288s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6029521; Time: 0.5355s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 150.78 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 32\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 32\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s32 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s32.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s32\"]*len(n4_eA_rF_vMed_s32)\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s32\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.5211298; Time: 33.8978s; Convergence value: 4.79e-01\n",
      "Epoch 2/5 --- Loss: 0.5298360; Time: 29.3004s; Convergence value: 2.22e-01\n",
      "Epoch 3/5 --- Loss: 0.5243396; Time: 34.9633s; Convergence value: 1.33e-01\n",
      "Epoch 4/5 --- Loss: 0.5216225; Time: 29.3338s; Convergence value: 8.42e-02\n",
      "Epoch 5/5 --- Loss: 0.5190710; Time: 29.3163s; Convergence value: 4.28e-03\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.7667s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5252s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5513s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5487s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5500s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5151s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5470s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5468s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5570s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6378747; Time: 0.5416s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 162.96 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 64\n",
    "\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 64\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s64 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s64.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s64\"]*len(n4_eA_rF_vMed_s64)\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s64\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.5421701; Time: 32.4095s; Convergence value: 4.58e-01\n",
      "Epoch 2/5 --- Loss: 0.5430398; Time: 30.6045s; Convergence value: 2.09e-01\n",
      "Epoch 3/5 --- Loss: 0.5407185; Time: 31.0489s; Convergence value: 1.23e-01\n",
      "Epoch 4/5 --- Loss: 0.5400136; Time: 30.7992s; Convergence value: 7.74e-02\n",
      "Epoch 5/5 --- Loss: 0.5396372; Time: 32.2932s; Convergence value: 9.82e-04\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.8164s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863686; Time: 0.5332s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863686; Time: 0.5687s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.5664s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863686; Time: 0.6195s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.5226s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.5246s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.5509s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.5495s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5863687; Time: 0.5333s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 163.52 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 128\n",
    "\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 200\n",
    "n_sessions = 128\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s128 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s128.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s128\"]*len(n4_eA_rF_vMed_s128)\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s128\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# run til HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 1.0352575; Time: 7.7921s; Convergence value: 3.53e-02\n",
      "Epoch 2/5 --- Loss: 0.6572930; Time: 7.1495s; Convergence value: 2.22e-01\n",
      "Epoch 3/5 --- Loss: 0.5730930; Time: 6.8980s; Convergence value: 1.69e-01\n",
      "Epoch 4/5 --- Loss: 0.5614052; Time: 6.7220s; Convergence value: 1.17e-01\n",
      "Epoch 5/5 --- Loss: 0.5523785; Time: 6.8675s; Convergence value: 8.80e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6678745; Time: 0.6247s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5222s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5353s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5329s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5697s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5316s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678745; Time: 0.5163s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5667s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678746; Time: 0.5142s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6678745; Time: 0.5357s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 41.03 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 128, trials 50\n",
    "\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 50 #200\n",
    "n_sessions = 128\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s128_t50 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s128_t50.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s128_t50\"]*len(n4_eA_rF_vMed_s128_t50)\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s128_t50\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6090691; Time: 6.9239s; Convergence value: 3.91e-01\n",
      "Epoch 2/5 --- Loss: 0.5869692; Time: 6.8027s; Convergence value: 1.90e-01\n",
      "Epoch 3/5 --- Loss: 0.5752981; Time: 6.6552s; Convergence value: 1.16e-01\n",
      "Epoch 4/5 --- Loss: 0.5748299; Time: 6.6199s; Convergence value: 7.35e-02\n",
      "Epoch 5/5 --- Loss: 0.5749742; Time: 6.6964s; Convergence value: 6.46e-03\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.6687s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.5791s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.6006s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.6344s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.6001s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.5785s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737438; Time: 0.5638s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.5703s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.5964s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5737439; Time: 0.6042s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 39.83 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 64, 50 trials\n",
    "\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 50 #200\n",
    "n_sessions = 64\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s64_t50 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s64_t50.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss'])\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s64_t50\"]*len(n4_eA_rF_vMed_s64_t50)\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s64_t50\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6217925; Time: 11.7978s; Convergence value: 3.78e-01\n",
      "Epoch 2/5 --- Loss: 0.6677495; Time: 6.8078s; Convergence value: 1.97e-01\n",
      "Epoch 3/5 --- Loss: 0.6336090; Time: 6.1318s; Convergence value: 1.30e-01\n",
      "Epoch 4/5 --- Loss: 0.5989357; Time: 7.6490s; Convergence value: 9.43e-02\n",
      "Epoch 5/5 --- Loss: 0.5883867; Time: 8.4393s; Convergence value: 2.84e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 0.9754s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 0.6978s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 0.6692s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 0.7836s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185135; Time: 0.8329s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185135; Time: 0.7908s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 1.3828s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 1.7611s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185136; Time: 1.5211s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6185135; Time: 1.4098s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 51.86 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_trials' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-38c09588029b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Ensemble\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"average\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdf_lenght\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Sessions\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_sessions\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdf_lenght\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Trials\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdf_lenght\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_trials' is not defined"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 32, 50 trials\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 50 #200\n",
    "n_sessions = 32\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s32_t50 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s32_t50.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['model', 'loss','Replacement', 'Submodels', 'Voting', 'Ensemble','Sessions', 'Trials'])\n",
    "\n",
    "df_lenght = len(n4_eA_rF_vMed_s32_t50)\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s32_t50\"] *df_lenght\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s32_t50\n",
    "\n",
    "df[\"Replacement\"] = [\"FALSE\"] *df_lenght\n",
    "df[\"Submodels\"] = [n_submodels] *df_lenght\n",
    "df[\"Voting\"] = [\"median\"] *df_lenght\n",
    "df[\"Ensemble\"] = [\"average\"] *df_lenght\n",
    "df[\"Sessions\"] = [n_sessions] *df_lenght\n",
    "df[\"Trials\"] = [n_trials_per_session] *df_lenght\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 1.2959127; Time: 10.8901s; Convergence value: 2.96e-01\n",
      "Epoch 2/5 --- Loss: 0.6493654; Time: 10.1214s; Convergence value: 4.87e-01\n",
      "Epoch 3/5 --- Loss: 0.6203648; Time: 8.9498s; Convergence value: 3.06e-01\n",
      "Epoch 4/5 --- Loss: 0.5834601; Time: 9.3614s; Convergence value: 2.13e-01\n",
      "Epoch 5/5 --- Loss: 0.5736622; Time: 10.2157s; Convergence value: 1.28e-01\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 1.0752s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 0.8873s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 0.7286s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 0.6864s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 1.0446s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 0.8164s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 0.6739s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039061; Time: 0.6630s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039060; Time: 0.6257s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6039061; Time: 0.6686s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 57.63 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "# 4 submodels, ensemble average, replacement false, n sessions = 32, 50 trials\n",
    "\n",
    "# train model \n",
    "train = True\n",
    "checkpoint = False\n",
    "data = False\n",
    "\n",
    "path_data = 'data/dataset_train.pkl'\n",
    "params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "# rnn parameters\n",
    "hidden_size = 4\n",
    "last_output = False\n",
    "last_state = False\n",
    "use_lstm = False\n",
    "\n",
    "# ensemble parameters\n",
    "evolution_interval = None\n",
    "sampling_replacement = False\n",
    "n_submodels = 4\n",
    "ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 5\n",
    "n_steps_per_call = 16  # None for full sequence\n",
    "batch_size = None  # None for one batch per epoch\n",
    "learning_rate = 1e-2\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "# ground truth parameters\n",
    "gen_alpha = .25\n",
    "gen_beta = 3\n",
    "forget_rate = 0.1  # possible values: 0., 0.1\n",
    "perseverance_bias = 0.\n",
    "correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "# environment parameters\n",
    "n_actions = 2\n",
    "sigma = 0.1\n",
    "n_trials_per_session = 50 #200\n",
    "n_sessions = 256\n",
    "correlated_reward = False\n",
    "non_binary_reward = False\n",
    "\n",
    "\n",
    "# tracked variables in the RNN\n",
    "x_train_list = ['xQf','xQr', 'xQc']\n",
    "control_list = ['ca','ca[k-1]', 'cr']\n",
    "sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if not data:\n",
    "  # setup\n",
    "  environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "  agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "  dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=n_trials_per_session,\n",
    "      n_sessions=n_sessions,\n",
    "      device=device)\n",
    "\n",
    "  dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "      agent=agent,\n",
    "      environment=environment,\n",
    "      n_trials_per_session=200,\n",
    "      n_sessions=1024,\n",
    "      device=device)\n",
    "  \n",
    "  params_path = rnn_utils.parameter_file_naming(\n",
    "      'params/params',\n",
    "      use_lstm,\n",
    "      last_output,\n",
    "      last_state,\n",
    "      gen_beta,\n",
    "      forget_rate,\n",
    "      perseverance_bias,\n",
    "      correlated_update,\n",
    "      non_binary_reward,\n",
    "      verbose=True,\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  # load data\n",
    "  with open(path_data, 'rb') as f:\n",
    "      dataset_train = pickle.load(f)\n",
    "\n",
    "if ensemble > -1 and n_submodels == 1:\n",
    "  Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "  ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "# define model\n",
    "if use_lstm:\n",
    "  model = rnn.LSTM(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      device=device,\n",
    "      ).to(device)\n",
    "else:\n",
    "  model = [rnn.RLRNN(\n",
    "      n_actions=n_actions, \n",
    "      hidden_size=hidden_size, \n",
    "      init_value=0.5,\n",
    "      last_output=last_output,\n",
    "      last_state=last_state,\n",
    "      device=device,\n",
    "      list_sindy_signals=sindy_feature_list,\n",
    "      ).to(device)\n",
    "           for _ in range(n_submodels)]\n",
    "\n",
    "optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "if checkpoint:\n",
    "    # load trained parameters\n",
    "    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "    state_dict_model = state_dict['model']\n",
    "    state_dict_optimizer = state_dict['optimizer']\n",
    "    if isinstance(state_dict_model, dict):\n",
    "      for m, o in zip(model, optimizer_rnn):\n",
    "        m.load_state_dict(state_dict_model)\n",
    "        o.load_state_dict(state_dict_optimizer)\n",
    "    elif isinstance(state_dict_model, list):\n",
    "        print('Loading ensemble model...')\n",
    "        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "            model[i].load_state_dict(state_dict_model_i)\n",
    "            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "    print('Loaded parameters.')\n",
    "\n",
    "if train:\n",
    "  \n",
    "  start_time = time.time()\n",
    "  \n",
    "  #Fit the hybrid RNN\n",
    "  print('Training the hybrid RNN...')\n",
    "  model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      optimizer=optimizer_rnn,\n",
    "      convergence_threshold=convergence_threshold,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      sampling_replacement=sampling_replacement,\n",
    "      evolution_interval=evolution_interval,\n",
    "      n_steps_per_call=n_steps_per_call,\n",
    "  )\n",
    "  \n",
    "\n",
    "  n4_eA_rF_vMed_s256_t50 = []\n",
    "\n",
    "  # validate model\n",
    "  print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "  for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "      model, _, loss = rnn_training.fit_model(\n",
    "          model=model,\n",
    "          dataset=dataset_test,\n",
    "          n_steps_per_call=1,\n",
    "      )\n",
    "      n4_eA_rF_vMed_s256_t50.append(float(loss))\n",
    "\n",
    "\n",
    "  print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "  \n",
    "\n",
    "  # save trained parameters  \n",
    "  state_dict = {\n",
    "    'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "    'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "  }\n",
    "  torch.save(state_dict, params_path)\n",
    "  \n",
    "  print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "else:\n",
    "  model, _, _ = rnn_training.fit_model(\n",
    "      model=model,\n",
    "      dataset=dataset_train,\n",
    "      epochs=0,\n",
    "      n_submodels=n_submodels,\n",
    "      ensemble_type=ensemble,\n",
    "      voting_type=voting_type,\n",
    "      verbose=True\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['model', 'loss','Replacement', 'Submodels', 'Voting', 'Ensemble','Sessions', 'Trials'])\n",
    "\n",
    "df_lenght = len(n4_eA_rF_vMed_s256_t50)\n",
    "\n",
    "df[\"model\"] = [\"n4_eA_rF_vMed_s256_t50\"] *df_lenght\n",
    "df[\"loss\"] = n4_eA_rF_vMed_s256_t50\n",
    "\n",
    "df[\"Replacement\"] = [\"FALSE\"] *df_lenght\n",
    "df[\"Submodels\"] = [n_submodels] *df_lenght\n",
    "df[\"Voting\"] = [\"median\"] *df_lenght\n",
    "df[\"Ensemble\"] = [\"average\"] *df_lenght\n",
    "df[\"Sessions\"] = [n_sessions] *df_lenght\n",
    "df[\"Trials\"] = [n_trials_per_session] *df_lenght\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"losses.csv\")\n",
    "losses = df1.append(df)\n",
    "\n",
    "losses.to_csv(\"losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.9459739; Time: 121.0738s; Convergence value: 5.40e-02\n",
      "Epoch 2/5 --- Loss: 0.6819629; Time: 130.7111s; Convergence value: 1.69e-01\n",
      "Epoch 3/5 --- Loss: 0.6902878; Time: 124.4173s; Convergence value: 1.06e-01\n",
      "Epoch 4/5 --- Loss: 0.6901503; Time: 126.3467s; Convergence value: 7.00e-02\n",
      "Epoch 5/5 --- Loss: 0.6881059; Time: 127.5003s; Convergence value: 4.66e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 1.1043s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 0.5810s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 1.0010s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 1.1528s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 1.0957s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 0.6544s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 0.7221s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 1.2143s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127562; Time: 1.1365s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6127561; Time: 0.9921s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 646.72 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.9950689; Time: 142.9123s; Convergence value: 4.93e-03\n",
      "Epoch 2/5 --- Loss: 0.6420822; Time: 140.9213s; Convergence value: 1.95e-01\n",
      "Epoch 3/5 --- Loss: 0.6409266; Time: 135.8377s; Convergence value: 1.19e-01\n",
      "Epoch 4/5 --- Loss: 0.6407392; Time: 152.5772s; Convergence value: 7.96e-02\n",
      "Epoch 5/5 --- Loss: 0.6409413; Time: 134.8240s; Convergence value: 5.92e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 1.2777s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.7855s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.7044s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367594; Time: 0.5896s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.7224s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.8066s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.6607s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.5973s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.5825s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6367595; Time: 0.5684s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 723.10 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7487606; Time: 137.9231s; Convergence value: 2.51e-01\n",
      "Epoch 2/5 --- Loss: 0.6225265; Time: 165.7926s; Convergence value: 1.83e-01\n",
      "Epoch 3/5 --- Loss: 0.6246484; Time: 148.1855s; Convergence value: 1.10e-01\n",
      "Epoch 4/5 --- Loss: 0.6234756; Time: 161.9918s; Convergence value: 7.09e-02\n",
      "Epoch 5/5 --- Loss: 0.6221551; Time: 202.5984s; Convergence value: 2.23e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6061666; Time: 1.1658s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.7221s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.5483s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061666; Time: 0.6477s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.6729s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.6267s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.7405s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061666; Time: 0.7196s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.6750s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6061667; Time: 0.7624s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 833.08 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7220324; Time: 320.3903s; Convergence value: 2.78e-01\n",
      "Epoch 2/5 --- Loss: 0.5748191; Time: 324.1784s; Convergence value: 2.07e-01\n",
      "Epoch 3/5 --- Loss: 0.5741509; Time: 304.2814s; Convergence value: 1.23e-01\n",
      "Epoch 4/5 --- Loss: 0.5726041; Time: 343.3454s; Convergence value: 7.97e-02\n",
      "Epoch 5/5 --- Loss: 0.5719589; Time: 338.1440s; Convergence value: 2.53e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 1.3090s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.6386s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.6443s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.6622s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.6683s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.7049s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.7070s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.7203s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.7194s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5918838; Time: 0.7757s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 1652.78 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7862026; Time: 348.4370s; Convergence value: 2.14e-01\n",
      "Epoch 2/5 --- Loss: 0.6528847; Time: 337.9797s; Convergence value: 1.70e-01\n",
      "Epoch 3/5 --- Loss: 0.6512772; Time: 333.0435s; Convergence value: 1.02e-01\n",
      "Epoch 4/5 --- Loss: 0.6509491; Time: 323.3679s; Convergence value: 6.58e-02\n",
      "Epoch 5/5 --- Loss: 0.6505870; Time: 377.4311s; Convergence value: 2.28e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 1.1055s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.7934s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.6995s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.7996s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.7496s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.6664s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.6670s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.6697s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015585; Time: 0.6710s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6015586; Time: 0.6651s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 1741.95 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6528990; Time: 362.8864s; Convergence value: 3.47e-01\n",
      "Epoch 2/5 --- Loss: 0.6083965; Time: 352.4954s; Convergence value: 1.82e-01\n",
      "Epoch 3/5 --- Loss: 0.6061023; Time: 345.1234s; Convergence value: 1.08e-01\n",
      "Epoch 4/5 --- Loss: 0.6059541; Time: 350.1379s; Convergence value: 6.84e-02\n",
      "Epoch 5/5 --- Loss: 0.6059936; Time: 349.1660s; Convergence value: 7.98e-03\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5861494; Time: 1.4826s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861494; Time: 0.7214s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861495; Time: 0.7425s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861495; Time: 0.7531s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861495; Time: 0.7274s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861494; Time: 0.7499s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861494; Time: 0.7327s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861494; Time: 0.7661s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861494; Time: 0.7196s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5861495; Time: 0.7642s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 1788.30 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n"
     ]
    }
   ],
   "source": [
    "nsubmod = [16,32]\n",
    "#trial_list = [32,64]\n",
    "sessionlist = [32, 64, 128]\n",
    "\n",
    "for m in nsubmod:\n",
    "    for ses in sessionlist:\n",
    "    \n",
    "        # train model \n",
    "        train = True\n",
    "        checkpoint = False\n",
    "        data = False\n",
    "\n",
    "        path_data = 'data/dataset_train.pkl'\n",
    "        params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "        # rnn parameters\n",
    "        hidden_size = 4\n",
    "        last_output = False\n",
    "        last_state = False\n",
    "        use_lstm = False\n",
    "\n",
    "        # ensemble parameters\n",
    "        evolution_interval = None\n",
    "        sampling_replacement = False\n",
    "        n_submodels = int(m)\n",
    "        ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "        voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "        # training parameters\n",
    "        epochs = 5\n",
    "        n_steps_per_call = 16  # None for full sequence\n",
    "        batch_size = None  # None for one batch per epoch\n",
    "        learning_rate = 1e-2\n",
    "        convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "        # ground truth parameters\n",
    "        gen_alpha = .25\n",
    "        gen_beta = 3\n",
    "        forget_rate = 0.1  # possible values: 0., 0.1\n",
    "        perseverance_bias = 0.\n",
    "        correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "        # environment parameters\n",
    "        n_actions = 2\n",
    "        sigma = 0.1\n",
    "        n_trials_per_session = 200 #200\n",
    "        n_sessions = int(ses)\n",
    "        correlated_reward = False\n",
    "        non_binary_reward = False\n",
    "\n",
    "\n",
    "        # tracked variables in the RNN\n",
    "        x_train_list = ['xQf','xQr', 'xQc']\n",
    "        control_list = ['ca','ca[k-1]', 'cr']\n",
    "        sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "        if not data:\n",
    "            # setup\n",
    "            environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "            agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "            dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "                    agent=agent,\n",
    "                    environment=environment,\n",
    "                    n_trials_per_session=n_trials_per_session,\n",
    "                    n_sessions=n_sessions,\n",
    "                    device=device)\n",
    "\n",
    "            dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "                    agent=agent,\n",
    "                    environment=environment,\n",
    "                    n_trials_per_session=200,\n",
    "                    n_sessions=1024,\n",
    "                    device=device)\n",
    "            \n",
    "            params_path = rnn_utils.parameter_file_naming(\n",
    "                    'params/params',\n",
    "                    use_lstm,\n",
    "                    last_output,\n",
    "                    last_state,\n",
    "                    gen_beta,\n",
    "                    forget_rate,\n",
    "                    perseverance_bias,\n",
    "                    correlated_update,\n",
    "                    non_binary_reward,\n",
    "                    verbose=True,\n",
    "            )\n",
    "    \n",
    "        else:\n",
    "            # load data\n",
    "            with open(path_data, 'rb') as f:\n",
    "                    dataset_train = pickle.load(f)\n",
    "\n",
    "        if ensemble > -1 and n_submodels == 1:\n",
    "            Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "            ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "        # define model\n",
    "        if use_lstm:\n",
    "            model = rnn.LSTM(\n",
    "                    n_actions=n_actions, \n",
    "                    hidden_size=hidden_size, \n",
    "                    init_value=0.5,\n",
    "                    device=device,\n",
    "                    ).to(device)\n",
    "        else:\n",
    "            model = [rnn.RLRNN(\n",
    "                    n_actions=n_actions, \n",
    "                    hidden_size=hidden_size, \n",
    "                    init_value=0.5,\n",
    "                    last_output=last_output,\n",
    "                    last_state=last_state,\n",
    "                    device=device,\n",
    "                    list_sindy_signals=sindy_feature_list,\n",
    "                    ).to(device)\n",
    "                        for _ in range(n_submodels)]\n",
    "\n",
    "        optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "        if checkpoint:\n",
    "            # load trained parameters\n",
    "            state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "            state_dict_model = state_dict['model']\n",
    "            state_dict_optimizer = state_dict['optimizer']\n",
    "            if isinstance(state_dict_model, dict):\n",
    "                for m, o in zip(model, optimizer_rnn):\n",
    "                    m.load_state_dict(state_dict_model)\n",
    "                    o.load_state_dict(state_dict_optimizer)\n",
    "            elif isinstance(state_dict_model, list):\n",
    "                print('Loading ensemble model...')\n",
    "                for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "                    model[i].load_state_dict(state_dict_model_i)\n",
    "                    optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "                rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "            print('Loaded parameters.')\n",
    "\n",
    "\n",
    "        if train:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            #Fit the hybrid RNN\n",
    "            print('Training the hybrid RNN...')\n",
    "            model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "                    model=model,\n",
    "                    dataset=dataset_train,\n",
    "                    optimizer=optimizer_rnn,\n",
    "                    convergence_threshold=convergence_threshold,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    n_submodels=n_submodels,\n",
    "                    ensemble_type=ensemble,\n",
    "                    voting_type=voting_type,\n",
    "                    sampling_replacement=sampling_replacement,\n",
    "                    evolution_interval=evolution_interval,\n",
    "                    n_steps_per_call=n_steps_per_call,\n",
    "            )\n",
    "            \n",
    "\n",
    "            model_name = []\n",
    "\n",
    "            # validate model\n",
    "            print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "            for _ in range(10):\n",
    "                with torch.no_grad():\n",
    "                    model, _, loss = rnn_training.fit_model(\n",
    "                            model=model,\n",
    "                            dataset=dataset_test,\n",
    "                            n_steps_per_call=1,\n",
    "                    )\n",
    "                model_name.append(float(loss))\n",
    "\n",
    "\n",
    "            print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "            \n",
    "\n",
    "            # save trained parameters  \n",
    "            state_dict = {\n",
    "                'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "                'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "            }\n",
    "            torch.save(state_dict, params_path)\n",
    "            \n",
    "            print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "        else:\n",
    "            model, _, _ = rnn_training.fit_model(\n",
    "                    model=model,\n",
    "                    dataset=dataset_train,\n",
    "                    epochs=0,\n",
    "                    n_submodels=n_submodels,\n",
    "                    ensemble_type=ensemble,\n",
    "                    voting_type=voting_type,\n",
    "                    verbose=True\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(columns=['model', 'loss','Replacement', 'Submodels', 'Voting', 'Ensemble','Sessions', 'Trials'])\n",
    "\n",
    "        df_lenght = len(model_name)\n",
    "\n",
    "        df[\"model\"] = [\"model_name\"] *df_lenght\n",
    "        df[\"loss\"] = model_name\n",
    "\n",
    "        df[\"Replacement\"] = [\"FALSE\"] *df_lenght\n",
    "        df[\"Submodels\"] = [n_submodels] *df_lenght\n",
    "        df[\"Voting\"] = [\"median\"] *df_lenght\n",
    "        df[\"Ensemble\"] = [\"average\"] *df_lenght\n",
    "        df[\"Sessions\"] = [n_sessions] *df_lenght\n",
    "        df[\"Trials\"] = [n_trials_per_session] *df_lenght\n",
    "\n",
    "\n",
    "        df1 = pd.read_csv(\"losses.csv\")\n",
    "        losses = df1.append(df)\n",
    "\n",
    "        losses.to_csv(\"losses.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6654302; Time: 8.2006s; Convergence value: 3.35e-01\n",
      "Epoch 2/5 --- Loss: 0.6795268; Time: 8.4795s; Convergence value: 1.60e-01\n",
      "Epoch 3/5 --- Loss: 0.6371923; Time: 7.5367s; Convergence value: 1.11e-01\n",
      "Epoch 4/5 --- Loss: 0.6334728; Time: 8.4136s; Convergence value: 7.19e-02\n",
      "Epoch 5/5 --- Loss: 0.6029378; Time: 7.6035s; Convergence value: 2.30e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.7512s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.5970s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.6560s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.6694s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.6446s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.6373s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.7330s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.6652s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115136; Time: 0.8254s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6115135; Time: 0.8003s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 47.44 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6166967; Time: 22.2089s; Convergence value: 3.83e-01\n",
      "Epoch 2/5 --- Loss: 0.5334430; Time: 18.6473s; Convergence value: 2.20e-01\n",
      "Epoch 3/5 --- Loss: 0.5339382; Time: 20.2330s; Convergence value: 1.30e-01\n",
      "Epoch 4/5 --- Loss: 0.5220556; Time: 20.6034s; Convergence value: 8.65e-02\n",
      "Epoch 5/5 --- Loss: 0.5862476; Time: 18.9632s; Convergence value: 3.87e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 1.0794s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.7984s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.6661s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.7605s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246643; Time: 0.6558s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.6653s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.6627s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.7252s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246642; Time: 0.7560s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6246641; Time: 0.7156s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 108.57 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.4343422; Time: 41.2503s; Convergence value: 5.66e-01\n",
      "Epoch 2/5 --- Loss: 0.5043964; Time: 40.1272s; Convergence value: 2.95e-01\n",
      "Epoch 3/5 --- Loss: 0.4236211; Time: 36.9075s; Convergence value: 2.07e-01\n",
      "Epoch 4/5 --- Loss: 0.4297431; Time: 38.2302s; Convergence value: 1.34e-01\n",
      "Epoch 5/5 --- Loss: 0.4294455; Time: 38.7449s; Convergence value: 3.14e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 1.2952s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.7260s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6558s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6553s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6306s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6743s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6790s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6286s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6575s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6411333; Time: 0.6557s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 203.57 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6061103; Time: 10.0920s; Convergence value: 3.94e-01\n",
      "Epoch 2/5 --- Loss: 0.5976177; Time: 8.7087s; Convergence value: 1.84e-01\n",
      "Epoch 3/5 --- Loss: 0.5742390; Time: 8.7505s; Convergence value: 1.17e-01\n",
      "Epoch 4/5 --- Loss: 0.5160986; Time: 10.2395s; Convergence value: 9.34e-02\n",
      "Epoch 5/5 --- Loss: 0.5785118; Time: 8.5552s; Convergence value: 4.36e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 1.4126s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 1.1692s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.8642s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.8610s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.9114s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.7517s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.6866s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.7642s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.6559s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6049687; Time: 0.6761s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 55.26 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.4747451; Time: 21.8144s; Convergence value: 5.25e-01\n",
      "Epoch 2/5 --- Loss: 0.4646818; Time: 19.4990s; Convergence value: 2.44e-01\n",
      "Epoch 3/5 --- Loss: 0.4598836; Time: 20.3806s; Convergence value: 1.45e-01\n",
      "Epoch 4/5 --- Loss: 0.3686654; Time: 18.7463s; Convergence value: 1.22e-01\n",
      "Epoch 5/5 --- Loss: 0.4660158; Time: 19.9660s; Convergence value: 6.05e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 1.2225s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.7546s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.8393s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.9150s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 1.0136s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.8109s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.8690s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.8552s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.7648s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5990269; Time: 0.8496s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 109.95 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.5986492; Time: 41.1216s; Convergence value: 4.01e-01\n",
      "Epoch 2/5 --- Loss: 0.5655928; Time: 41.3473s; Convergence value: 2.00e-01\n",
      "Epoch 3/5 --- Loss: 0.6527856; Time: 40.5614s; Convergence value: 1.53e-01\n",
      "Epoch 4/5 --- Loss: 0.5636315; Time: 42.8696s; Convergence value: 1.28e-01\n",
      "Epoch 5/5 --- Loss: 0.6241904; Time: 41.5470s; Convergence value: 6.98e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6034306; Time: 1.3095s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034305; Time: 0.8170s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034305; Time: 0.7122s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034305; Time: 0.6895s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034305; Time: 0.7298s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034306; Time: 0.7162s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034306; Time: 0.7168s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034306; Time: 0.8040s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034305; Time: 0.7042s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6034305; Time: 0.7074s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 216.44 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.8966226; Time: 9.6372s; Convergence value: 1.03e-01\n",
      "Epoch 2/5 --- Loss: 0.6514475; Time: 8.8475s; Convergence value: 1.81e-01\n",
      "Epoch 3/5 --- Loss: 0.6188019; Time: 10.7201s; Convergence value: 1.22e-01\n",
      "Epoch 4/5 --- Loss: 0.6255897; Time: 14.0071s; Convergence value: 8.30e-02\n",
      "Epoch 5/5 --- Loss: 0.5848816; Time: 9.8944s; Convergence value: 6.36e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5978756; Time: 0.8537s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6946s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6596s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6945s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.7349s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6144s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6473s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6358s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6473s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5978757; Time: 0.6495s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 60.16 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 1.0255031; Time: 22.5188s; Convergence value: 2.55e-02\n",
      "Epoch 2/5 --- Loss: 0.5838856; Time: 21.5640s; Convergence value: 2.52e-01\n",
      "Epoch 3/5 --- Loss: 0.6072054; Time: 20.2768s; Convergence value: 1.63e-01\n",
      "Epoch 4/5 --- Loss: 0.6248244; Time: 18.6994s; Convergence value: 1.15e-01\n",
      "Epoch 5/5 --- Loss: 0.5856825; Time: 19.9255s; Convergence value: 9.67e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 1.0827s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.7538s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.6516s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.6263s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.6898s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.6737s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.9193s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.6848s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.7375s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5890836; Time: 0.6383s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 111.01 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6138448; Time: 42.1148s; Convergence value: 3.86e-01\n",
      "Epoch 2/5 --- Loss: 0.6340764; Time: 46.9081s; Convergence value: 1.87e-01\n",
      "Epoch 3/5 --- Loss: 0.5926662; Time: 44.7238s; Convergence value: 1.26e-01\n",
      "Epoch 4/5 --- Loss: 0.5896270; Time: 50.2838s; Convergence value: 8.14e-02\n",
      "Epoch 5/5 --- Loss: 0.5800257; Time: 74.5127s; Convergence value: 1.66e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 1.8838s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 1.0200s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983224; Time: 1.0083s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983224; Time: 1.1348s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 1.1370s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 1.0959s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 0.9829s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 0.9615s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983225; Time: 0.9331s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983224; Time: 0.7994s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 270.93 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.9223101; Time: 46.0736s; Convergence value: 7.77e-02\n",
      "Epoch 2/5 --- Loss: 0.6988631; Time: 46.7892s; Convergence value: 1.57e-01\n",
      "Epoch 3/5 --- Loss: 0.6851316; Time: 44.5420s; Convergence value: 1.01e-01\n",
      "Epoch 4/5 --- Loss: 0.6785002; Time: 44.8802s; Convergence value: 6.86e-02\n",
      "Epoch 5/5 --- Loss: 0.6995339; Time: 46.6549s; Convergence value: 4.91e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 1.3954s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550464; Time: 1.0116s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 0.9648s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 0.8971s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 1.1201s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 1.0702s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 1.2726s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 1.2767s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 1.1089s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6550465; Time: 0.9165s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 241.38 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6371704; Time: 97.9114s; Convergence value: 3.63e-01\n",
      "Epoch 2/5 --- Loss: 0.6008990; Time: 86.5337s; Convergence value: 1.85e-01\n",
      "Epoch 3/5 --- Loss: 0.6036252; Time: 79.5892s; Convergence value: 1.10e-01\n",
      "Epoch 4/5 --- Loss: 0.5956798; Time: 80.4331s; Convergence value: 7.19e-02\n",
      "Epoch 5/5 --- Loss: 0.5744274; Time: 85.9168s; Convergence value: 1.59e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 1.4073s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263238; Time: 0.8973s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263238; Time: 0.8666s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 0.8335s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 0.9097s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263238; Time: 1.0237s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 0.9335s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 1.0669s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 0.9322s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6263237; Time: 0.9697s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 444.07 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6106151; Time: 194.5181s; Convergence value: 3.89e-01\n",
      "Epoch 2/5 --- Loss: 0.5140316; Time: 182.7314s; Convergence value: 2.30e-01\n",
      "Epoch 3/5 --- Loss: 0.5199968; Time: 201.4320s; Convergence value: 1.38e-01\n",
      "Epoch 4/5 --- Loss: 0.5214492; Time: 151.8688s; Convergence value: 8.85e-02\n",
      "Epoch 5/5 --- Loss: 0.4972749; Time: 184.4193s; Convergence value: 2.59e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 2.7344s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.8739s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.6846s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.9413s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 1.5930s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 1.0972s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.8419s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.8096s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.7884s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6327336; Time: 0.9902s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 935.85 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6957800; Time: 35.7068s; Convergence value: 3.04e-01\n",
      "Epoch 2/5 --- Loss: 0.6028810; Time: 38.0415s; Convergence value: 1.89e-01\n",
      "Epoch 3/5 --- Loss: 0.5752223; Time: 37.3159s; Convergence value: 1.23e-01\n",
      "Epoch 4/5 --- Loss: 0.5796538; Time: 40.6956s; Convergence value: 8.05e-02\n",
      "Epoch 5/5 --- Loss: 0.5724951; Time: 42.2045s; Convergence value: 2.52e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 1.2742s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916315; Time: 0.9460s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 0.9192s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 0.8655s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 0.7829s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 0.7416s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916315; Time: 0.7088s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916315; Time: 0.6593s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 0.7536s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5916316; Time: 0.8224s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 204.18 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6957165; Time: 81.1968s; Convergence value: 3.04e-01\n",
      "Epoch 2/5 --- Loss: 0.5748665; Time: 80.8049s; Convergence value: 2.04e-01\n",
      "Epoch 3/5 --- Loss: 0.5700719; Time: 81.9927s; Convergence value: 1.23e-01\n",
      "Epoch 4/5 --- Loss: 0.5440117; Time: 80.2845s; Convergence value: 8.76e-02\n",
      "Epoch 5/5 --- Loss: 0.5643684; Time: 81.6124s; Convergence value: 3.52e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 1.5071s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.8216s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.9593s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.7640s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.9165s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.8100s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.7560s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.6792s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.7957s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5986772; Time: 0.8737s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 418.84 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7431702; Time: 172.4687s; Convergence value: 2.57e-01\n",
      "Epoch 2/5 --- Loss: 0.5337180; Time: 177.3255s; Convergence value: 2.31e-01\n",
      "Epoch 3/5 --- Loss: 0.4926824; Time: 203.2310s; Convergence value: 1.55e-01\n",
      "Epoch 4/5 --- Loss: 0.4882785; Time: 177.8141s; Convergence value: 1.02e-01\n",
      "Epoch 5/5 --- Loss: 0.4940718; Time: 177.6183s; Convergence value: 4.72e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6334018; Time: 1.5755s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334018; Time: 0.6906s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7089s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334018; Time: 0.6994s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7091s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7494s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7629s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7320s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7263s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6334017; Time: 0.7172s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 927.66 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6339934; Time: 36.6198s; Convergence value: 3.66e-01\n",
      "Epoch 2/5 --- Loss: 0.6083356; Time: 36.6294s; Convergence value: 1.80e-01\n",
      "Epoch 3/5 --- Loss: 0.5732176; Time: 50.8210s; Convergence value: 1.20e-01\n",
      "Epoch 4/5 --- Loss: 0.5846481; Time: 36.7390s; Convergence value: 8.03e-02\n",
      "Epoch 5/5 --- Loss: 0.5645628; Time: 38.2947s; Convergence value: 2.20e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.9803s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.6871s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.8314s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.6668s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.7059s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.7083s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.6517s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.6998s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983277; Time: 0.6847s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5983278; Time: 0.6436s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 207.85 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.6552030; Time: 91.4444s; Convergence value: 3.45e-01\n",
      "Epoch 2/5 --- Loss: 0.5869275; Time: 76.0719s; Convergence value: 1.94e-01\n",
      "Epoch 3/5 --- Loss: 0.5953753; Time: 85.8703s; Convergence value: 1.18e-01\n",
      "Epoch 4/5 --- Loss: 0.5840843; Time: 70.8108s; Convergence value: 7.87e-02\n",
      "Epoch 5/5 --- Loss: 0.5813166; Time: 80.4905s; Convergence value: 1.73e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 1.6264s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8323s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188924; Time: 0.8584s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8890s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8504s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8623s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8182s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8009s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8306s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6188923; Time: 0.8154s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 418.45 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7802613; Time: 187.5013s; Convergence value: 2.20e-01\n",
      "Epoch 2/5 --- Loss: 0.5834225; Time: 197.2060s; Convergence value: 2.07e-01\n",
      "Epoch 3/5 --- Loss: 0.5760636; Time: 214.9546s; Convergence value: 1.27e-01\n",
      "Epoch 4/5 --- Loss: 0.5752089; Time: 219.1065s; Convergence value: 8.27e-02\n",
      "Epoch 5/5 --- Loss: 0.5811088; Time: 188.0146s; Convergence value: 3.66e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6187381; Time: 1.4759s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187381; Time: 1.3272s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187380; Time: 0.9712s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187381; Time: 0.8871s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187380; Time: 0.8790s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187380; Time: 0.8757s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187381; Time: 0.8669s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187380; Time: 0.9063s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187380; Time: 0.8392s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6187381; Time: 0.8712s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 1024.49 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7291435; Time: 64.3528s; Convergence value: 2.71e-01\n",
      "Epoch 2/5 --- Loss: 0.5935630; Time: 66.3651s; Convergence value: 1.97e-01\n",
      "Epoch 3/5 --- Loss: 0.5769725; Time: 69.6462s; Convergence value: 1.24e-01\n",
      "Epoch 4/5 --- Loss: 0.5768457; Time: 77.7181s; Convergence value: 7.99e-02\n",
      "Epoch 5/5 --- Loss: 0.5662387; Time: 72.6319s; Convergence value: 2.99e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.9673s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933757; Time: 0.8162s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7947s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7529s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7905s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7556s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7661s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7522s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7653s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.5933756; Time: 0.7656s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 361.10 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n",
      "Epoch 1/5 --- Loss: 0.7528229; Time: 158.1936s; Convergence value: 2.47e-01\n",
      "Epoch 2/5 --- Loss: 0.6683888; Time: 128.3593s; Convergence value: 1.58e-01\n",
      "Epoch 3/5 --- Loss: 0.6826010; Time: 162.1947s; Convergence value: 9.97e-02\n",
      "Epoch 4/5 --- Loss: 0.6766346; Time: 166.0735s; Convergence value: 6.59e-02\n",
      "Epoch 5/5 --- Loss: 0.6844866; Time: 164.8579s; Convergence value: 2.15e-02\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "\n",
      "Validating the trained hybrid RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.6050492; Time: 1.0488s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050491; Time: 0.7694s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050492; Time: 0.7238s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050491; Time: 0.7884s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050491; Time: 0.7279s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050492; Time: 0.7357s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050492; Time: 0.7489s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050491; Time: 0.7488s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050491; Time: 0.7640s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Epoch 1/1 --- Loss: 0.6050491; Time: 0.7513s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Training took 793.45 seconds.\n",
      "Saved RNN parameters to file params/params_rnn_b3_f01.pkl.\n",
      "Automatically generated name for model parameter file: params/params_rnn_b3_f01.pkl.\n",
      "Training the hybrid RNN...\n"
     ]
    }
   ],
   "source": [
    "nsubmod = [4, 16, 32]\n",
    "sessionlist = [32, 64, 128]\n",
    "tri = [50,100,200]\n",
    "rep = [True]\n",
    "\n",
    "for m in nsubmod:\n",
    "    for ses in sessionlist:\n",
    "        for t in tri:\n",
    "            for r in rep:\n",
    "    \n",
    "                # train model \n",
    "                train = True\n",
    "                checkpoint = False\n",
    "                data = False\n",
    "\n",
    "                path_data = 'data/dataset_train.pkl'\n",
    "                params_path = 'params/params_lstm_b3.pkl'  # overwritten if data is False (adapted to the ground truth model)\n",
    "\n",
    "                # rnn parameters\n",
    "                hidden_size = 4\n",
    "                last_output = False\n",
    "                last_state = False\n",
    "                use_lstm = False\n",
    "\n",
    "                # ensemble parameters\n",
    "                evolution_interval = None\n",
    "                sampling_replacement = bool(r)\n",
    "                n_submodels = int(m)\n",
    "                ensemble = rnn_training.ensemble_types.AVERAGE\n",
    "                voting_type = rnn.EnsembleRNN.MEDIAN  # necessary if ensemble==True\n",
    "\n",
    "\n",
    "                # training parameters\n",
    "                epochs = 5\n",
    "                n_steps_per_call = 16  # None for full sequence\n",
    "                batch_size = None  # None for one batch per epoch\n",
    "                learning_rate = 1e-2\n",
    "                convergence_threshold = 1e-6\n",
    "\n",
    "\n",
    "                # ground truth parameters\n",
    "                gen_alpha = .25\n",
    "                gen_beta = 3\n",
    "                forget_rate = 0.1  # possible values: 0., 0.1\n",
    "                perseverance_bias = 0.\n",
    "                correlated_update = False  # possible values: True, False\n",
    "\n",
    "\n",
    "                # environment parameters\n",
    "                n_actions = 2\n",
    "                sigma = 0.1\n",
    "                n_trials_per_session = int(t) #200\n",
    "                n_sessions = int(ses)\n",
    "                correlated_reward = False\n",
    "                non_binary_reward = False\n",
    "\n",
    "\n",
    "                # tracked variables in the RNN\n",
    "                x_train_list = ['xQf','xQr', 'xQc']\n",
    "                control_list = ['ca','ca[k-1]', 'cr']\n",
    "                sindy_feature_list = x_train_list + control_list\n",
    "\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "                if not data:\n",
    "                    # setup\n",
    "                    environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_reward=non_binary_reward, correlated_reward=correlated_reward)\n",
    "                    agent = bandits.AgentQ(gen_alpha, gen_beta, n_actions, forget_rate, perseverance_bias, correlated_update)  \n",
    "\n",
    "                    dataset_train, experiment_list_train = bandits.create_dataset(\n",
    "                            agent=agent,\n",
    "                            environment=environment,\n",
    "                            n_trials_per_session=n_trials_per_session,\n",
    "                            n_sessions=n_sessions,\n",
    "                            device=device)\n",
    "\n",
    "                    dataset_test, experiment_list_test = bandits.create_dataset(\n",
    "                            agent=agent,\n",
    "                            environment=environment,\n",
    "                            n_trials_per_session=200,\n",
    "                            n_sessions=1024,\n",
    "                            device=device)\n",
    "                    \n",
    "                    params_path = rnn_utils.parameter_file_naming(\n",
    "                            'params/params',\n",
    "                            use_lstm,\n",
    "                            last_output,\n",
    "                            last_state,\n",
    "                            gen_beta,\n",
    "                            forget_rate,\n",
    "                            perseverance_bias,\n",
    "                            correlated_update,\n",
    "                            non_binary_reward,\n",
    "                            verbose=True,\n",
    "                    )\n",
    "            \n",
    "                else:\n",
    "                    # load data\n",
    "                    with open(path_data, 'rb') as f:\n",
    "                            dataset_train = pickle.load(f)\n",
    "\n",
    "                if ensemble > -1 and n_submodels == 1:\n",
    "                    Warning('Ensemble is actived but n_submodels is set to 1. Deactivating ensemble...')\n",
    "                    ensemble = rnn_training.ensemble_types.NONE\n",
    "\n",
    "                # define model\n",
    "                if use_lstm:\n",
    "                    model = rnn.LSTM(\n",
    "                            n_actions=n_actions, \n",
    "                            hidden_size=hidden_size, \n",
    "                            init_value=0.5,\n",
    "                            device=device,\n",
    "                            ).to(device)\n",
    "                else:\n",
    "                    model = [rnn.RLRNN(\n",
    "                            n_actions=n_actions, \n",
    "                            hidden_size=hidden_size, \n",
    "                            init_value=0.5,\n",
    "                            last_output=last_output,\n",
    "                            last_state=last_state,\n",
    "                            device=device,\n",
    "                            list_sindy_signals=sindy_feature_list,\n",
    "                            ).to(device)\n",
    "                                for _ in range(n_submodels)]\n",
    "\n",
    "                optimizer_rnn = [torch.optim.Adam(m.parameters(), lr=learning_rate) for m in model]\n",
    "\n",
    "                if checkpoint:\n",
    "                    # load trained parameters\n",
    "                    state_dict = torch.load(params_path, map_location=torch.device('cpu'))\n",
    "                    state_dict_model = state_dict['model']\n",
    "                    state_dict_optimizer = state_dict['optimizer']\n",
    "                    if isinstance(state_dict_model, dict):\n",
    "                        for m, o in zip(model, optimizer_rnn):\n",
    "                            m.load_state_dict(state_dict_model)\n",
    "                            o.load_state_dict(state_dict_optimizer)\n",
    "                    elif isinstance(state_dict_model, list):\n",
    "                        print('Loading ensemble model...')\n",
    "                        for i, state_dict_model_i, state_dict_optim_i in zip(range(n_submodels), state_dict_model, state_dict_optimizer):\n",
    "                            model[i].load_state_dict(state_dict_model_i)\n",
    "                            optimizer_rnn[i].load_state_dict(state_dict_optim_i)\n",
    "                        rnn = rnn.EnsembleRNN(model, voting_type=voting_type)\n",
    "                    print('Loaded parameters.')\n",
    "\n",
    "\n",
    "                if train:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    #Fit the hybrid RNN\n",
    "                    print('Training the hybrid RNN...')\n",
    "                    model, optimizer_rnn, _ = rnn_training.fit_model(\n",
    "                            model=model,\n",
    "                            dataset=dataset_train,\n",
    "                            optimizer=optimizer_rnn,\n",
    "                            convergence_threshold=convergence_threshold,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            n_submodels=n_submodels,\n",
    "                            ensemble_type=ensemble,\n",
    "                            voting_type=voting_type,\n",
    "                            sampling_replacement=sampling_replacement,\n",
    "                            evolution_interval=evolution_interval,\n",
    "                            n_steps_per_call=n_steps_per_call,\n",
    "                    )\n",
    "                    \n",
    "\n",
    "                    model_name = []\n",
    "\n",
    "                    # validate model\n",
    "                    print('\\nValidating the trained hybrid RNN on a test dataset...')\n",
    "\n",
    "                    for _ in range(10):\n",
    "                        with torch.no_grad():\n",
    "                            model, _, loss = rnn_training.fit_model(\n",
    "                                    model=model,\n",
    "                                    dataset=dataset_test,\n",
    "                                    n_steps_per_call=1,\n",
    "                            )\n",
    "                            model_name.append(float(loss))\n",
    "\n",
    "\n",
    "                    print(f'Training took {time.time() - start_time:.2f} seconds.')\n",
    "                    \n",
    "\n",
    "                    # save trained parameters  \n",
    "                    state_dict = {\n",
    "                        'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n",
    "                        'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n",
    "                    }\n",
    "                    torch.save(state_dict, params_path)\n",
    "                    \n",
    "                    print(f'Saved RNN parameters to file {params_path}.')\n",
    "\n",
    "                else:\n",
    "                    model, _, _ = rnn_training.fit_model(\n",
    "                            model=model,\n",
    "                            dataset=dataset_train,\n",
    "                            epochs=0,\n",
    "                            n_submodels=n_submodels,\n",
    "                            ensemble_type=ensemble,\n",
    "                            voting_type=voting_type,\n",
    "                            verbose=True\n",
    "                    )\n",
    "\n",
    "                df = pd.DataFrame(columns=['model', 'loss','Replacement', 'Submodels', 'Voting', 'Ensemble','Sessions', 'Trials'])\n",
    "\n",
    "                df_lenght = len(model_name)\n",
    "\n",
    "                df[\"model\"] = [\"model_name\"] *df_lenght\n",
    "                df[\"loss\"] = model_name\n",
    "\n",
    "                df[\"Replacement\"] = [\"TRUE\"] *df_lenght\n",
    "                df[\"Submodels\"] = [n_submodels] *df_lenght\n",
    "                df[\"Voting\"] = [\"median\"] *df_lenght\n",
    "                df[\"Ensemble\"] = [\"average\"] *df_lenght\n",
    "                df[\"Sessions\"] = [n_sessions] *df_lenght\n",
    "                df[\"Trials\"] = [n_trials_per_session] *df_lenght\n",
    "\n",
    "\n",
    "                df1 = pd.read_csv(\"losses.csv\")\n",
    "                losses = df1.append(df)\n",
    "\n",
    "                losses.to_csv(\"losses.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
